{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aee99cabddfd4f7f80add94c460caf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eea0ccafce54aef9aae5e59c4069ac3",
              "IPY_MODEL_27af1a4331ee481a920e814a23b98dd0",
              "IPY_MODEL_4896ef5847544cde9d43b369ca480bc6"
            ],
            "layout": "IPY_MODEL_c3831c2c8fc34925bbe1512f19fc326d"
          }
        },
        "5eea0ccafce54aef9aae5e59c4069ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6becdebd347a4b858469403daad75021",
            "placeholder": "​",
            "style": "IPY_MODEL_10eb4bacfc814d958cd095d415813ae4",
            "value": "Batches: 100%"
          }
        },
        "27af1a4331ee481a920e814a23b98dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc285ec2dac429d9857b28d3ebeca85",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eb1335391ac413aaafed57a74e5b901",
            "value": 5
          }
        },
        "4896ef5847544cde9d43b369ca480bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0329bd0e9a3428c92a17ed2b7d2d8bd",
            "placeholder": "​",
            "style": "IPY_MODEL_9a09ad3b98ba4d95aac93772e54605a4",
            "value": " 5/5 [00:37&lt;00:00,  5.69s/it]"
          }
        },
        "c3831c2c8fc34925bbe1512f19fc326d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6becdebd347a4b858469403daad75021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10eb4bacfc814d958cd095d415813ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fc285ec2dac429d9857b28d3ebeca85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb1335391ac413aaafed57a74e5b901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0329bd0e9a3428c92a17ed2b7d2d8bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a09ad3b98ba4d95aac93772e54605a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "SgUoNkvvXdxk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch sentence-transformers faiss-cpu pypdf2 langchain openai python-dotenv chromadb python-docx tiktoken google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "# Core libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# OpenAI integration\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "# Document processing\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from io import BytesIO\n",
        "\n",
        "# Google Drive integration\n",
        "from google.colab import drive, files\n",
        "import zipfile\n",
        "\n",
        "# Vector database\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from typing import Any, Dict\n",
        "\n",
        "# Display and visualization\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h6kWLNW5Xmu1",
        "outputId": "64ea7979-ba14-49b7-bac6-46adcabd79e4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"✅ Google Drive mounted successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error mounting Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "def upload_files():\n",
        "    \"\"\"Upload files directly to Colab\"\"\"\n",
        "    print(\"📁 Please upload your documents (PDF, DOCX, TXT files)\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Create documents directory\n",
        "    os.makedirs(\"documents\", exist_ok=True)\n",
        "\n",
        "    # Move uploaded files to documents directory\n",
        "    for filename in uploaded.keys():\n",
        "        os.rename(filename, f\"documents/{filename}\")\n",
        "        print(f\"✅ Moved {filename} to documents/\")\n",
        "\n",
        "    return list(uploaded.keys())\n",
        "\n",
        "def list_drive_files(drive_path=\"/content/drive/MyDrive\"):\n",
        "    \"\"\"List files in Google Drive\"\"\"\n",
        "    try:\n",
        "        files_found = []\n",
        "        for root, dirs, files in os.walk(drive_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.pdf', '.docx', '.txt', '.doc')):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    rel_path = os.path.relpath(full_path, drive_path)\n",
        "                    files_found.append((file, full_path, rel_path))\n",
        "        return files_found\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error listing drive files: {e}\")\n",
        "        return []\n",
        "\n",
        "def copy_from_drive(file_paths):\n",
        "    \"\"\"Copy selected files from Google Drive to local documents folder\"\"\"\n",
        "    os.makedirs(\"documents\", exist_ok=True)\n",
        "    copied_files = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            filename = os.path.basename(file_path)\n",
        "            destination = f\"documents/{filename}\"\n",
        "\n",
        "            # Copy file\n",
        "            with open(file_path, 'rb') as src, open(destination, 'wb') as dst:\n",
        "                dst.write(src.read())\n",
        "\n",
        "            copied_files.append(filename)\n",
        "            print(f\"✅ Copied: {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error copying {file_path}: {e}\")\n",
        "\n",
        "    return copied_files\n",
        "\n",
        "# Document Upload Interface\n",
        "print(\"📂 DOCUMENT UPLOAD OPTIONS:\")\n",
        "print(\"1. Mount Google Drive and select files\")\n",
        "print(\"2. Upload files directly\")\n",
        "print(\"3. Skip (if documents already in documents/ folder)\")\n",
        "\n",
        "choice = input(\"\\nEnter your choice (1/2/3): \").strip()\n",
        "\n",
        "uploaded_files = []\n",
        "\n",
        "if choice == \"1\":\n",
        "    if mount_drive():\n",
        "        print(\"\\n📁 Scanning Google Drive for documents...\")\n",
        "        drive_files = list_drive_files()\n",
        "\n",
        "        if drive_files:\n",
        "            print(f\"\\n📋 Found {len(drive_files)} documents in Google Drive:\")\n",
        "            for i, (filename, full_path, rel_path) in enumerate(drive_files, 1):\n",
        "                print(f\"{i}. {filename} ({rel_path})\")\n",
        "\n",
        "            # Let user select files\n",
        "            selection = input(f\"\\nEnter file numbers to use (e.g., 1,3,5) or 'all' for all files: \").strip()\n",
        "\n",
        "            if selection.lower() == 'all':\n",
        "                selected_paths = [full_path for _, full_path, _ in drive_files]\n",
        "            else:\n",
        "                try:\n",
        "                    indices = [int(x.strip()) - 1 for x in selection.split(',')]\n",
        "                    selected_paths = [drive_files[i][1] for i in indices if 0 <= i < len(drive_files)]\n",
        "                except:\n",
        "                    print(\"❌ Invalid selection. Using all files.\")\n",
        "                    selected_paths = [full_path for _, full_path, _ in drive_files]\n",
        "\n",
        "            uploaded_files = copy_from_drive(selected_paths)\n",
        "        else:\n",
        "            print(\"❌ No documents found in Google Drive\")\n",
        "\n",
        "elif choice == \"2\":\n",
        "    uploaded_files = upload_files()\n",
        "\n",
        "elif choice == \"3\":\n",
        "    # Check if documents folder exists and has files\n",
        "    if os.path.exists(\"documents\"):\n",
        "        uploaded_files = [f for f in os.listdir(\"documents\")\n",
        "                         if f.lower().endswith(('.pdf', '.docx', '.txt', '.doc'))]\n",
        "        if uploaded_files:\n",
        "            print(f\"✅ Found {len(uploaded_files)} documents in documents/ folder\")\n",
        "        else:\n",
        "            print(\"❌ No documents found in documents/ folder\")\n",
        "    else:\n",
        "        print(\"❌ documents/ folder not found\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Invalid choice\")\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"⚠️ No documents available. Please upload documents first.\")\n",
        "else:\n",
        "    print(f\"\\n🎉 Ready to process {len(uploaded_files)} documents!\")\n",
        "    for file in uploaded_files:\n",
        "        print(f\"  📄 {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EZD3e_JhX2gB",
        "outputId": "1794964a-5879-4510-d495-7092701e8e9b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 DOCUMENT UPLOAD OPTIONS:\n",
            "1. Mount Google Drive and select files\n",
            "2. Upload files directly\n",
            "3. Skip (if documents already in documents/ folder)\n",
            "\n",
            "Enter your choice (1/2/3): 3\n",
            "✅ Found 3 documents in documents/ folder\n",
            "\n",
            "🎉 Ready to process 3 documents!\n",
            "  📄 2005.11401v4.pdf\n",
            "  📄 2005.14165v4.pdf\n",
            "  📄 1706.03762v7.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    \"\"\"Represents a chunk of text from a document\"\"\"\n",
        "    text: str\n",
        "    source: str\n",
        "    page_number: int\n",
        "    chunk_id: str\n",
        "    metadata: Dict\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document loading, cleaning, and chunking\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 1000, overlap: int = 200):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.supported_formats = ['.pdf', '.docx', '.txt', '.doc']\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        text_pages = []\n",
        "\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    text = page.extract_text()\n",
        "                    if text.strip():\n",
        "                        text_pages.append((text, page_num + 1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading PDF {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return text_pages\n",
        "\n",
        "    def extract_text_from_docx(self, docx_path: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Extract text from DOCX file\"\"\"\n",
        "        try:\n",
        "            doc = Document(docx_path)\n",
        "            text_pages = []\n",
        "            current_text = \"\"\n",
        "\n",
        "            for paragraph in doc.paragraphs:\n",
        "                current_text += paragraph.text + \"\\n\"\n",
        "\n",
        "            if current_text.strip():\n",
        "                text_pages.append((current_text, 1))\n",
        "\n",
        "            return text_pages\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading DOCX {docx_path}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def extract_text_from_txt(self, txt_path: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Extract text from TXT file\"\"\"\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                return [(text, 1)] if text.strip() else []\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(txt_path, 'r', encoding='latin-1') as file:\n",
        "                    text = file.read()\n",
        "                    return [(text, 1)] if text.strip() else []\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error reading TXT {txt_path}: {str(e)}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading TXT {txt_path}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def extract_text_from_file(self, file_path: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Extract text from any supported file format\"\"\"\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        if file_ext == '.pdf':\n",
        "            return self.extract_text_from_pdf(file_path)\n",
        "        elif file_ext in ['.docx', '.doc']:\n",
        "            return self.extract_text_from_docx(file_path)\n",
        "        elif file_ext == '.txt':\n",
        "            return self.extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            print(f\"❌ Unsupported file format: {file_ext}\")\n",
        "            return []\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove special characters but keep punctuation\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:()\\-\\'\\\"\"]', '', text)\n",
        "        # Fix common PDF extraction issues\n",
        "        text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)  # Fix hyphenated words\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def create_chunks(self, text: str, source: str, page_number: int) -> List[DocumentChunk]:\n",
        "        \"\"\"Create overlapping chunks from text\"\"\"\n",
        "        chunks = []\n",
        "        words = text.split()\n",
        "\n",
        "        if len(words) < 10:  # Skip very short texts\n",
        "            return chunks\n",
        "\n",
        "        for i in range(0, len(words), self.chunk_size - self.overlap):\n",
        "            chunk_words = words[i:i + self.chunk_size]\n",
        "            chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "            if len(chunk_text.strip()) > 50:  # Only keep substantial chunks\n",
        "                chunk_id = f\"{source}_page_{page_number}_chunk_{len(chunks)}\"\n",
        "\n",
        "                chunk = DocumentChunk(\n",
        "                    text=chunk_text,\n",
        "                    source=source,\n",
        "                    page_number=page_number,\n",
        "                    chunk_id=chunk_id,\n",
        "                    metadata={\n",
        "                        'word_count': len(chunk_words),\n",
        "                        'char_count': len(chunk_text),\n",
        "                        'file_type': os.path.splitext(source)[1]\n",
        "                    }\n",
        "                )\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents_folder: str = \"documents\") -> List[DocumentChunk]:\n",
        "        \"\"\"Process all documents in folder and return chunks\"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        if not os.path.exists(documents_folder):\n",
        "            print(f\"❌ Documents folder '{documents_folder}' not found\")\n",
        "            return all_chunks\n",
        "\n",
        "        files = [f for f in os.listdir(documents_folder)\n",
        "                if os.path.splitext(f)[1].lower() in self.supported_formats]\n",
        "\n",
        "        if not files:\n",
        "            print(f\"❌ No supported documents found in '{documents_folder}'\")\n",
        "            return all_chunks\n",
        "\n",
        "        print(f\"🔄 Processing {len(files)} documents...\")\n",
        "\n",
        "        for filename in files:\n",
        "            print(f\"📄 Processing: {filename}\")\n",
        "\n",
        "            file_path = os.path.join(documents_folder, filename)\n",
        "\n",
        "            # Extract text from file\n",
        "            text_pages = self.extract_text_from_file(file_path)\n",
        "\n",
        "            if not text_pages:\n",
        "                print(f\"⚠️ No text extracted from {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Process each page\n",
        "            doc_chunks = 0\n",
        "            for text, page_num in text_pages:\n",
        "                cleaned_text = self.clean_text(text)\n",
        "                if cleaned_text:\n",
        "                    chunks = self.create_chunks(cleaned_text, filename, page_num)\n",
        "                    all_chunks.extend(chunks)\n",
        "                    doc_chunks += len(chunks)\n",
        "\n",
        "            print(f\"✅ Created {doc_chunks} chunks from {filename}\")\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "# Process documents\n",
        "print(\"\\n🔄 Processing your documents...\")\n",
        "processor = DocumentProcessor(chunk_size=800, overlap=150)\n",
        "document_chunks = processor.process_documents(\"documents\")\n",
        "\n",
        "if document_chunks:\n",
        "    print(f\"\\n📊 Processing Summary:\")\n",
        "    print(f\"Total chunks created: {len(document_chunks)}\")\n",
        "\n",
        "    # Group by source\n",
        "    source_counts = {}\n",
        "    for chunk in document_chunks:\n",
        "        source_counts[chunk.source] = source_counts.get(chunk.source, 0) + 1\n",
        "\n",
        "    for source, count in source_counts.items():\n",
        "        print(f\"  📄 {source}: {count} chunks\")\n",
        "else:\n",
        "    print(\"❌ No chunks created. Please check your documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oor8xI1AX8cP",
        "outputId": "28be25a5-4f91-4d8a-bb91-7e6bd90a6984"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing your documents...\n",
            "🔄 Processing 3 documents...\n",
            "📄 Processing: 2005.11401v4.pdf\n",
            "✅ Created 22 chunks from 2005.11401v4.pdf\n",
            "📄 Processing: 2005.14165v4.pdf\n",
            "✅ Created 93 chunks from 2005.14165v4.pdf\n",
            "📄 Processing: 1706.03762v7.pdf\n",
            "✅ Created 15 chunks from 1706.03762v7.pdf\n",
            "\n",
            "📊 Processing Summary:\n",
            "Total chunks created: 130\n",
            "  📄 2005.11401v4.pdf: 22 chunks\n",
            "  📄 2005.14165v4.pdf: 93 chunks\n",
            "  📄 1706.03762v7.pdf: 15 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        "    \"\"\"Handles text embeddings using sentence transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        print(f\"🔄 Loading embedding model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "        print(f\"✅ Embedding model loaded. Dimension: {self.dimension}\")\n",
        "\n",
        "    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
        "        \"\"\"Encode texts to embeddings\"\"\"\n",
        "        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
        "\n",
        "    def encode_single(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Encode single text to embedding\"\"\"\n",
        "        return self.model.encode([text])[0]\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"FAISS-based vector store for efficient similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: EmbeddingModel):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "        self.embeddings = None\n",
        "\n",
        "    def build_index(self, chunks: List[DocumentChunk]) -> None:\n",
        "        \"\"\"Build FAISS index from document chunks\"\"\"\n",
        "        if not chunks:\n",
        "            print(\"❌ No chunks provided for indexing\")\n",
        "            return\n",
        "\n",
        "        print(\"🔄 Building vector index...\")\n",
        "\n",
        "        self.chunks = chunks\n",
        "        texts = [chunk.text for chunk in chunks]\n",
        "\n",
        "        # Generate embeddings\n",
        "        print(\"🔄 Generating embeddings...\")\n",
        "        self.embeddings = self.embedding_model.encode(texts)\n",
        "\n",
        "        # Build FAISS index\n",
        "        self.index = faiss.IndexFlatIP(self.embedding_model.dimension)  # Inner product for cosine similarity\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "        print(f\"✅ Vector index built with {len(chunks)} chunks\")\n",
        "\n",
        "    def search(self, query: str, k: int = 5) -> List[Tuple[DocumentChunk, float]]:\n",
        "        \"\"\"Search for most relevant chunks\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call build_index first.\")\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode_single(query).reshape(1, -1)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx < len(self.chunks):\n",
        "                results.append((self.chunks[idx], float(score)))\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "HlEpF6-OYAOz"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_gemini():\n",
        "    \"\"\"Setup Gemini API key\"\"\"\n",
        "    print(\"🔑 Setting up Gemini API...\")\n",
        "\n",
        "    # Method 1: Try to get from environment\n",
        "    api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"\\n📝 Gemini API Key Setup:\")\n",
        "        print(\"You can get your API key from: https://makersuite.google.com/app/apikey\")\n",
        "        print(\"Or from Google Cloud Console: https://console.cloud.google.com/\")\n",
        "        api_key = input(\"Enter your Google Gemini API key: \").strip()\n",
        "\n",
        "        if api_key:\n",
        "            os.environ['GOOGLE_API_KEY'] = api_key\n",
        "            print(\"✅ API key set successfully!\")\n",
        "        else:\n",
        "            print(\"⚠️ No API key provided. Will use local models as fallback.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        # Configure Gemini\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Test the API by listing models\n",
        "        models = list(genai.list_models())\n",
        "        print(f\"✅ Gemini API connection successful! Found {len(models)} available models.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gemini API setup failed: {e}\")\n",
        "        print(\"Will use local models as fallback.\")\n",
        "        return None\n",
        "\n",
        "# Setup Gemini (optional)\n",
        "gemini_available = setup_gemini()\n",
        "\n",
        "# Initialize embedding model and vector store (only if we have chunks)\n",
        "if document_chunks:\n",
        "    embedding_model = EmbeddingModel(\"all-MiniLM-L6-v2\")\n",
        "    vector_store = VectorStore(embedding_model)\n",
        "    vector_store.build_index(document_chunks)\n",
        "    print(\"✅ Vector store ready!\")\n",
        "else:\n",
        "    print(\"⚠️ Skipping vector store creation - no document chunks available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "aee99cabddfd4f7f80add94c460caf7d",
            "5eea0ccafce54aef9aae5e59c4069ac3",
            "27af1a4331ee481a920e814a23b98dd0",
            "4896ef5847544cde9d43b369ca480bc6",
            "c3831c2c8fc34925bbe1512f19fc326d",
            "6becdebd347a4b858469403daad75021",
            "10eb4bacfc814d958cd095d415813ae4",
            "8fc285ec2dac429d9857b28d3ebeca85",
            "8eb1335391ac413aaafed57a74e5b901",
            "f0329bd0e9a3428c92a17ed2b7d2d8bd",
            "9a09ad3b98ba4d95aac93772e54605a4"
          ]
        },
        "id": "qyIOi1gVYC_e",
        "outputId": "a3d0c29d-b7b4-417c-e9f5-c6b0873082d1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Setting up Gemini API...\n",
            "✅ Gemini API connection successful! Found 53 available models.\n",
            "🔄 Loading embedding model: all-MiniLM-L6-v2\n",
            "✅ Embedding model loaded. Dimension: 384\n",
            "🔄 Building vector index...\n",
            "🔄 Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aee99cabddfd4f7f80add94c460caf7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vector index built with 130 chunks\n",
            "✅ Vector store ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiAnswerGenerator:\n",
        "    \"\"\"Answer generator using Google's Gemini models\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"gemini-1.5-flash\"):\n",
        "        self.model_name = model_name\n",
        "        try:\n",
        "            self.model = genai.GenerativeModel(model_name)\n",
        "            print(f\"✅ Gemini Answer Generator initialized with {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to initialize Gemini model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Estimate token count (approximate for Gemini)\"\"\"\n",
        "        # Rough estimation: 1 token ≈ 4 characters for most languages\n",
        "        return len(text) // 4\n",
        "\n",
        "    def truncate_context(self, context: str, max_chars: int = 30000) -> str:\n",
        "        \"\"\"Truncate context to fit within character limits\"\"\"\n",
        "        if len(context) <= max_chars:\n",
        "            return context\n",
        "        return context[:max_chars] + \"...\"\n",
        "\n",
        "    def generate_answer(self, query: str, context_chunks: List[DocumentChunk]) -> str:\n",
        "        \"\"\"Generate answer using Gemini API\"\"\"\n",
        "\n",
        "        if not context_chunks:\n",
        "            return \"No relevant context found to answer this question.\"\n",
        "\n",
        "        # Prepare context from chunks\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(context_chunks[:5], 1):  # Use top 5 chunks\n",
        "            source_name = os.path.splitext(chunk.source)[0]\n",
        "            context_parts.append(f\"Document {i} ({source_name}, Page {chunk.page_number}):\\n{chunk.text}\\n\")\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        # Truncate context if too long\n",
        "        context = self.truncate_context(context)\n",
        "\n",
        "        # Create prompt for Gemini\n",
        "        prompt = f\"\"\"You are a helpful AI assistant that answers questions based on provided document contexts.\n",
        "\n",
        "Instructions:\n",
        "1. Answer the question using only the information provided in the context\n",
        "2. Be accurate and cite specific documents when possible\n",
        "3. If the context doesn't contain enough information, say so clearly\n",
        "4. Provide comprehensive answers when possible\n",
        "5. Use a professional and informative tone\n",
        "\n",
        "Context from documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a detailed answer based on the context above.\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Generate response using Gemini\n",
        "            response = self.model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=0.3,  # Lower temperature for more focused answers\n",
        "                    max_output_tokens=500,\n",
        "                    top_p=0.9,\n",
        "                    top_k=40\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if response.text:\n",
        "                return response.text.strip()\n",
        "            else:\n",
        "                return self._fallback_answer(query, context_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Gemini API error: {e}\")\n",
        "            return self._fallback_answer(query, context_chunks)\n",
        "\n",
        "    def _fallback_answer(self, query: str, context_chunks: List[DocumentChunk]) -> str:\n",
        "        \"\"\"Fallback answer generation\"\"\"\n",
        "        relevant_sentences = []\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        for chunk in context_chunks[:2]:\n",
        "            sentences = re.split(r'[.!?]+', chunk.text)\n",
        "            for sentence in sentences:\n",
        "                if len(sentence.strip()) > 20:\n",
        "                    sentence_words = set(sentence.lower().split())\n",
        "                    overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "                    if overlap > 0:\n",
        "                        relevant_sentences.append((sentence.strip(), overlap))\n",
        "\n",
        "        relevant_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if relevant_sentences:\n",
        "            answer_parts = [sent[0] for sent in relevant_sentences[:3]]\n",
        "            return \". \".join(answer_parts) + \".\"\n",
        "        else:\n",
        "            return \"Based on the available context, I cannot provide a specific answer to this question.\"\n",
        "\n",
        "class LocalAnswerGenerator:\n",
        "    \"\"\"Fallback local answer generator using transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"distilgpt2\"):\n",
        "        print(f\"🔄 Loading local answer generation model: {model_name}\")\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            tokenizer=model_name,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        print(\"✅ Local answer generation model loaded\")\n",
        "\n",
        "    def generate_answer(self, query: str, context_chunks: List[DocumentChunk]) -> str:\n",
        "        \"\"\"Generate answer using local model\"\"\"\n",
        "\n",
        "        if not context_chunks:\n",
        "            return \"No relevant context found to answer this question.\"\n",
        "\n",
        "        # Prepare context\n",
        "        context_texts = []\n",
        "        for chunk in context_chunks[:3]:\n",
        "            source_name = os.path.splitext(chunk.source)[0]\n",
        "            context_texts.append(f\"[{source_name}] {chunk.text[:300]}...\")\n",
        "\n",
        "        context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Based on the following document excerpts, answer the question accurately:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=len(prompt.split()) + 100,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.generator.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            generated_text = response[0]['generated_text']\n",
        "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "            # Clean up the answer\n",
        "            answer = re.sub(r'^[^\\w]*', '', answer)\n",
        "\n",
        "            return answer if answer else self._fallback_answer(query, context_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {e}\")\n",
        "            return self._fallback_answer(query, context_chunks)\n",
        "\n",
        "    def _fallback_answer(self, query: str, context_chunks: List[DocumentChunk]) -> str:\n",
        "        \"\"\"Simple extractive answer\"\"\"\n",
        "        relevant_sentences = []\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        for chunk in context_chunks[:2]:\n",
        "            sentences = re.split(r'[.!?]+', chunk.text)\n",
        "            for sentence in sentences:\n",
        "                if len(sentence.strip()) > 20:\n",
        "                    sentence_words = set(sentence.lower().split())\n",
        "                    overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "                    if overlap > 0:\n",
        "                        relevant_sentences.append((sentence.strip(), overlap))\n",
        "\n",
        "        relevant_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if relevant_sentences:\n",
        "            answer_parts = [sent[0] for sent in relevant_sentences[:3]]\n",
        "            return \". \".join(answer_parts) + \".\"\n",
        "        else:\n",
        "            return \"Based on the available context, I cannot provide a specific answer to this question.\"\n",
        "\n",
        "# Initialize answer generator\n",
        "if document_chunks:\n",
        "    if gemini_available:\n",
        "        print(\"🔄 Initializing Gemini answer generator...\")\n",
        "        try:\n",
        "            answer_generator = GeminiAnswerGenerator(\"gemini-1.5-flash\")\n",
        "            print(\"✅ Using Google Gemini 1.5 Flash for answer generation!\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to initialize Gemini: {e}\")\n",
        "            print(\"🔄 Falling back to local model...\")\n",
        "            answer_generator = LocalAnswerGenerator()\n",
        "    else:\n",
        "        print(\"🔄 Initializing local answer generator...\")\n",
        "        answer_generator = LocalAnswerGenerator()\n",
        "        print(\"✅ Using local model for answer generation\")\n",
        "else:\n",
        "    answer_generator = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CBsQ5SriYDy4",
        "outputId": "ca3ca0ce-14e1-4f44-c655-90867cc2eef3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Initializing Gemini answer generator...\n",
            "✅ Gemini Answer Generator initialized with gemini-1.5-flash\n",
            "✅ Using Google Gemini 1.5 Flash for answer generation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system for question answering\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: Any, answer_generator: Any):\n",
        "        self.vector_store = vector_store\n",
        "        self.answer_generator = answer_generator\n",
        "\n",
        "    def answer_question(self, query: str, k: int = 5) -> Dict:\n",
        "        \"\"\"Answer a question using RAG approach\"\"\"\n",
        "\n",
        "        # Step 1: Retrieve relevant chunks\n",
        "        retrieved_chunks = self.vector_store.search(query, k=k)\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        chunks_only = [chunk for chunk, score in retrieved_chunks]\n",
        "        answer = self.answer_generator.generate_answer(query, chunks_only)\n",
        "\n",
        "        # Step 3: Prepare response with source attribution\n",
        "        sources = []\n",
        "        for chunk, score in retrieved_chunks:\n",
        "            sources.append({\n",
        "                'document': chunk.source,\n",
        "                'page': chunk.page_number,\n",
        "                'relevance_score': score,\n",
        "                'file_type': chunk.metadata.get('file_type', 'unknown'),\n",
        "                'excerpt': chunk.text[:200] + \"...\" if len(chunk.text) > 200 else chunk.text\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'answer': answer,\n",
        "            'sources': sources,\n",
        "            'num_sources': len(sources)\n",
        "        }\n",
        "\n",
        "    def display_answer(self, result: Dict) -> None:\n",
        "        \"\"\"Display answer in a formatted way\"\"\"\n",
        "\n",
        "        display(Markdown(f\"## 🤔 Question: {result['query']}\"))\n",
        "        display(Markdown(f\"## 💡 Answer:\\n{result['answer']}\"))\n",
        "\n",
        "        display(Markdown(\"## 📚 Sources:\"))\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            display(Markdown(f\"\"\"\n",
        "**{i}. {source['document']}**\n",
        "- Page: {source['page']}\n",
        "- File Type: {source['file_type']}\n",
        "- Relevance Score: {source['relevance_score']:.3f}\n",
        "- Excerpt: *{source['excerpt']}*\n",
        "            \"\"\"))\n",
        "\n",
        "# Initialize complete RAG system (only if we have everything)\n",
        "if document_chunks and 'vector_store' in locals() and 'answer_generator' in locals():\n",
        "    rag_system = RAGSystem(vector_store, answer_generator)\n",
        "    print(\"🎉 RAG System initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eN9Jc8K1YKie",
        "outputId": "5fdf5ffd-1be4-4542-fb98-cf5fb1d6d07f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 RAG System initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Start a question-answer loop with the RAG system\"\"\"\n",
        "    print(\"\\n🧠 Ask me anything about your documents!\")\n",
        "    print(\"Type 'quit' to exit, or 'docs' to see the list of documents.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Display unique document sources\n",
        "    unique_sources = set(chunk.source for chunk in document_chunks)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\n❓ Your question: \").strip()\n",
        "\n",
        "        if query.lower() == 'quit':\n",
        "            print(\"👋 Exiting. Happy learning!\")\n",
        "            break\n",
        "        elif query.lower() == 'docs':\n",
        "            print(f\"\\n📄 Available documents ({len(unique_sources)}):\")\n",
        "            for doc in sorted(unique_sources):\n",
        "                chunks_count = len([c for c in document_chunks if c.source == doc])\n",
        "                print(f\"  • {doc} ({chunks_count} chunks)\")\n",
        "            continue\n",
        "        elif not query:\n",
        "            print(\"⚠️ Please enter a valid question.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(\"\\n🔍 Getting your answer...\")\n",
        "            result = rag_system.answer_question(query)\n",
        "\n",
        "            print(f\"\\n✅ **Answer:**\\n{result['answer']}\")\n",
        "\n",
        "            print(f\"\\n📚 **Sources ({len(result['sources'])}):**\")\n",
        "            for i, source in enumerate(result['sources'][:3], 1):\n",
        "                print(f\"{i}. {source['document']} (Page {source['page']}, Score: {source['relevance_score']:.3f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n"
      ],
      "metadata": {
        "id": "MvyQAUJXYQQ-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "🎯 RAG SYSTEM IS READY!\n",
        "═════════════════════════════════════════════════════════════════════\n",
        "\n",
        "📊 SYSTEM STATUS:\n",
        "• Documents loaded: {len(set(chunk.source for chunk in document_chunks))}\n",
        "• Total chunks: {len(document_chunks)}\n",
        "• Vector index: ✅ Ready\n",
        "• LLM backend: {\"🌟 Google Gemini 1.5 Flash\" if isinstance(answer_generator, GeminiAnswerGenerator) else \"🏠 Local DistilGPT-2\"}\n",
        "• Interface: Interactive Q&A enabled\n",
        "\n",
        "🧠 HOW TO USE:\n",
        "• Ask your questions using `interactive_query()`\n",
        "• Or call `rag_system.answer_question(\"Your question here\")` in code\n",
        "• Use `rag_system.display_answer(result)` for full formatted output\n",
        "\n",
        "🚀 START BY ASKING:\n",
        "Try: `interactive_query()` and ask questions like:\n",
        "→ What are the key findings?\n",
        "→ What methods were used in the study?\n",
        "→ Summarize document XYZ.\n",
        "\n",
        "🧪 TESTING SYSTEM WITH A SAMPLE QUESTION:\n",
        "\"\"\")\n",
        "\n",
        "sample_question = \"What are the main topics covered in these documents?\"\n",
        "print(f\"📝 Sample Question: {sample_question}\\n\" + \"=\" * 50)\n",
        "\n",
        "try:\n",
        "    result = rag_system.answer_question(sample_question)\n",
        "    rag_system.display_answer(result)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in sample query: {e}\")\n",
        "\n",
        "print(\"\\n🎉 All set! Start exploring your documents by running `interactive_query()`\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1533
        },
        "id": "azLlFU6iYT5Z",
        "outputId": "2d5b0a30-d8ef-4ead-804c-2807f1be8457"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 RAG SYSTEM IS READY!\n",
            "═════════════════════════════════════════════════════════════════════\n",
            "\n",
            "📊 SYSTEM STATUS:\n",
            "• Documents loaded: 3\n",
            "• Total chunks: 130\n",
            "• Vector index: ✅ Ready\n",
            "• LLM backend: 🌟 Google Gemini 1.5 Flash\n",
            "• Interface: Interactive Q&A enabled\n",
            "\n",
            "🧠 HOW TO USE:\n",
            "• Ask your questions using `interactive_query()`\n",
            "• Or call `rag_system.answer_question(\"Your question here\")` in code\n",
            "• Use `rag_system.display_answer(result)` for full formatted output\n",
            "\n",
            "🚀 START BY ASKING:\n",
            "Try: `interactive_query()` and ask questions like:\n",
            "→ What are the key findings?\n",
            "→ What methods were used in the study?\n",
            "→ Summarize document XYZ.\n",
            "\n",
            "🧪 TESTING SYSTEM WITH A SAMPLE QUESTION:\n",
            "\n",
            "📝 Sample Question: What are the main topics covered in these documents?\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 🤔 Question: What are the main topics covered in these documents?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 💡 Answer:\nThe provided documents cover several topics related to natural language processing (NLP) and large language models.  Specifically:\n\n* **Document 1 (2005.11401v4, Page 7):** This document focuses on the performance evaluation of a Retrieval Augmented Generation (RAG) model.  It discusses the model's accuracy in various tasks, including Jeopardy question generation, and compares its performance to other models like BART.  The document also analyzes the model's retrieval mechanism and its ability to generate diverse and factually accurate responses.  It mentions the works of Ernest Hemingway, specifically *A Farewell to Arms* and *The Sun Also Rises*, as examples used in the model's evaluation.\n\n* **Document 2 (2005.14165v4, Page 66):** This document appears to present results from various NLP tasks, including common sense reasoning, question answering, reading comprehension, and natural language inference (ANLI).  However, the provided text only shows figure references (Figures H.6-H.9) and lacks specific details about the results.\n\n* **Document 3 (2005.14165v4, Page 50):** This document describes a dataset used for evaluating a model's ability to understand conversational nuances across different cultures.  It provides examples of questions and answers related to appropriate conversation topics in various cultures (Latin America, France, the United States, Japan, China, Korea, and the Middle East).\n\n* **Document 4 (2005.14165v4, Page 72):** This document lists numerous citations of publications related to natural language processing.  These citations cover a wide range of topics within NLP, including word embeddings, question answering, commonsense reasoning, and bias in language models.\n\n* **Document 5 (2005.14165v4, Page 44):** This document discusses the methodology used to assess data contamination in a large language model training dataset.  It describes techniques for identifying and filtering overlapping content between training and test sets to prevent overfitting.  The document also analyzes the impact of data contamination on model performance and discusses challenges in accurately identifying contaminated data."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 📚 Sources:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**1. 2005.11401v4.pdf**\n- Page: 7\n- File Type: .pdf\n- Relevance Score: 0.320\n- Excerpt: *Document 1 : his works are considered classics of American literature ... His wartime experiences formed the basis for his novel A Farewell to Arms (1929) ... Document 2 : ... artists of the 1920s Los...*\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**2. 2005.14165v4.pdf**\n- Page: 66\n- File Type: .pdf\n- Relevance Score: 0.313\n- Excerpt: *Figure H.6: All results for all Common Sense Reasoning tasks. Figure H.7: All results for all QA tasks. Figure H.8: All results for all Reading Comprehension tasks. Figure H.9: All results for all ANL...*\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**3. 2005.14165v4.pdf**\n- Page: 50\n- File Type: .pdf\n- Relevance Score: 0.310\n- Excerpt: *G Details of Task Phrasing and Speciﬁcations The following ﬁgures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this s...*\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**4. 2005.14165v4.pdf**\n- Page: 72\n- File Type: .pdf\n- Relevance Score: 0.297\n- Excerpt: *MBXS17 Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems , pages 62946305, 2017...*\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**5. 2005.14165v4.pdf**\n- Page: 44\n- File Type: .pdf\n- Relevance Score: 0.292\n- Excerpt: *removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a...*\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 All set! Start exploring your documents by running `interactive_query()`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "9k5NXSsmoJOy",
        "outputId": "77200506-67bb-432b-da3b-4b9c9c16bd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Ask me anything about your documents!\n",
            "Type 'quit' to exit, or 'docs' to see the list of documents.\n",
            "============================================================\n",
            "\n",
            "❓ Your question: What are the two sub-layers in each encoder layer of the Transformer model?\n",
            "\n",
            "🔍 Getting your answer...\n",
            "\n",
            "✅ **Answer:**\n",
            "Based on Document 1 (1706.03762v7, Page 3), each encoder layer in the Transformer model contains two sub-layers.  The first is a multi-head self-attention mechanism, and the second is a position-wise fully connected feed-forward network.\n",
            "\n",
            "📚 **Sources (5):**\n",
            "1. 1706.03762v7.pdf (Page 3, Score: 0.686)\n",
            "2. 1706.03762v7.pdf (Page 5, Score: 0.458)\n",
            "3. 1706.03762v7.pdf (Page 8, Score: 0.430)\n"
          ]
        }
      ]
    }
  ]
}