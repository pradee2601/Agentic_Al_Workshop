{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbqMVPmhpSIQFHuD7qT9e5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradee2601/Agentic_Al_Workshop/blob/main/Ai_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FvVQFR7JUPS",
        "outputId": "65b8af2b-1947-4a1c-d2dc-6ed0d5915129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.7.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.9.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2025.4.26)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from tavily import TavilyClient\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "Yy1kJ4YGJ2NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"AIzaSyCmWBbnEX69h9APv-RzIgF_Bwnp646f48I\"\n",
        "TAVILY_API_KEY = \"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\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Tavily client\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
      ],
      "metadata": {
        "id": "iwy6_xfdMkYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicReActAgent:\n",
        "    \"\"\"\n",
        "    Dynamic ReAct (Reasoning + Acting) Agent for Web Research\n",
        "\n",
        "    This agent dynamically:\n",
        "    1. Takes user input (topic/question)\n",
        "    2. Generates relevant research questions automatically\n",
        "    3. Searches web for each question\n",
        "    4. Provides comprehensive answers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gemini_model=\"gemini-1.5-flash\"):\n",
        "        \"\"\"Initialize the Dynamic ReAct agent with Gemini model\"\"\"\n",
        "        self.model = genai.GenerativeModel(gemini_model)\n",
        "        self.research_results = {}\n",
        "        self.original_input = \"\"\n",
        "\n",
        "    def generate_dynamic_questions(self, user_input, num_questions=5):\n",
        "        \"\"\"\n",
        "        REASONING PHASE: Dynamically generate research questions based on user input\n",
        "\n",
        "        Args:\n",
        "            user_input (str): User's topic or question\n",
        "            num_questions (int): Number of questions to generate\n",
        "\n",
        "        Returns:\n",
        "            list: Dynamically generated research questions\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ§  REASONING PHASE: Analyzing input and generating dynamic questions...\")\n",
        "        print(f\"ğŸ“ User Input: '{user_input}'\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        A user has provided this input: \"{user_input}\"\n",
        "\n",
        "        Your task is to generate {num_questions} specific, searchable research questions that will help provide comprehensive information about their input.\n",
        "\n",
        "        Rules for question generation:\n",
        "        1. If the input is a broad topic (like \"Climate Change\"), create questions covering different aspects\n",
        "        2. If the input is already a question, create related questions that provide context and deeper understanding\n",
        "        3. Each question should be specific and web-searchable\n",
        "        4. Questions should build upon each other to create comprehensive coverage\n",
        "        5. Include both factual and analytical questions\n",
        "        6. Make questions current and relevant\n",
        "\n",
        "        Examples:\n",
        "        - If input is \"Artificial Intelligence\": Generate questions about applications, benefits, risks, future, etc.\n",
        "        - If input is \"How does solar energy work?\": Generate questions about the process, efficiency, costs, applications, etc.\n",
        "        - If input is \"Tesla\": Generate questions about company history, products, market position, technology, etc.\n",
        "\n",
        "        Generate EXACTLY {num_questions} questions, numbered 1-{num_questions}.\n",
        "        Return only the questions, no additional text.\n",
        "\n",
        "        Format:\n",
        "        1. [First specific question]\n",
        "        2. [Second specific question]\n",
        "        ...\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            questions_text = response.text.strip()\n",
        "\n",
        "            # Parse questions from the response\n",
        "            questions = []\n",
        "            for line in questions_text.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if line and (line[0].isdigit() or line.startswith('â€¢') or line.startswith('-')):\n",
        "                    # Clean up the question text\n",
        "                    question = line.split('.', 1)[-1].strip() if '.' in line else line.strip()\n",
        "                    if question and len(question) > 10:  # Ensure meaningful questions\n",
        "                        questions.append(question)\n",
        "\n",
        "            print(f\"âœ… Generated {len(questions)} dynamic research questions:\")\n",
        "            for i, q in enumerate(questions, 1):\n",
        "                print(f\"   {i}. {q}\")\n",
        "\n",
        "            return questions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error generating dynamic questions: {e}\")\n",
        "            # Fallback questions based on input type\n",
        "            return self._generate_fallback_questions(user_input)\n",
        "\n",
        "    def _generate_fallback_questions(self, user_input):\n",
        "        \"\"\"Generate basic fallback questions if LLM fails\"\"\"\n",
        "        fallback = [\n",
        "            f\"What is {user_input}?\",\n",
        "            f\"How does {user_input} work?\",\n",
        "            f\"What are the benefits of {user_input}?\",\n",
        "            f\"What are current trends in {user_input}?\",\n",
        "            f\"What is the future of {user_input}?\"\n",
        "        ]\n",
        "        return fallback\n",
        "\n",
        "    def search_web_advanced(self, query, max_results=4):\n",
        "        \"\"\"\n",
        "        ACTING PHASE: Advanced web search with multiple strategies\n",
        "\n",
        "        Args:\n",
        "            query (str): Search query\n",
        "            max_results (int): Maximum number of results\n",
        "\n",
        "        Returns:\n",
        "            list: Enhanced search results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"ğŸ” Searching: {query}\")\n",
        "\n",
        "            # Use Tavily for web search\n",
        "            response = tavily_client.search(\n",
        "                query=query,\n",
        "                search_depth=\"advanced\",\n",
        "                max_results=max_results,\n",
        "                include_answer=True  # Get Tavily's answer if available\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "            for result in response.get('results', []):\n",
        "                results.append({\n",
        "                    'title': result.get('title', 'No title'),\n",
        "                    'content': result.get('content', 'No content'),\n",
        "                    'url': result.get('url', 'No URL'),\n",
        "                    'score': result.get('score', 0)\n",
        "                })\n",
        "\n",
        "            # Also include Tavily's direct answer if available\n",
        "            tavily_answer = response.get('answer', '')\n",
        "            if tavily_answer:\n",
        "                results.insert(0, {\n",
        "                    'title': 'Direct Answer Summary',\n",
        "                    'content': tavily_answer,\n",
        "                    'url': 'Tavily AI Summary',\n",
        "                    'score': 1.0\n",
        "                })\n",
        "\n",
        "            print(f\"âœ… Found {len(results)} sources\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error searching for '{query}': {e}\")\n",
        "            return []\n",
        "\n",
        "    def synthesize_answer(self, question, search_results):\n",
        "        \"\"\"\n",
        "        REASONING PHASE: Synthesize comprehensive answer from search results\n",
        "\n",
        "        Args:\n",
        "            question (str): The research question\n",
        "            search_results (list): Web search results\n",
        "\n",
        "        Returns:\n",
        "            dict: Synthesized answer with metadata\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ§  REASONING PHASE: Synthesizing answer for '{question[:50]}...'\")\n",
        "\n",
        "        if not search_results:\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': 'No relevant information found for this question.',\n",
        "                'sources': [],\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "        # Prepare context from search results\n",
        "        context = \"\"\n",
        "        sources = []\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            context += f\"Source {i} ({result.get('score', 'N/A')}): {result['title']}\\n\"\n",
        "            context += f\"Content: {result['content']}\\n\"\n",
        "            context += f\"URL: {result['url']}\\n\\n\"\n",
        "\n",
        "            sources.append({\n",
        "                'title': result['title'],\n",
        "                'url': result['url'],\n",
        "                'content': result['content'][:300] + \"...\" if len(result['content']) > 300 else result['content'],\n",
        "                'relevance_score': result.get('score', 0)\n",
        "            })\n",
        "\n",
        "        synthesis_prompt = f\"\"\"\n",
        "        Research Question: {question}\n",
        "\n",
        "        Based on the following search results, provide a comprehensive, well-structured answer:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Instructions:\n",
        "        1. Provide a direct, detailed answer to the question\n",
        "        2. Structure your response with clear main points\n",
        "        3. Use specific information from the search results\n",
        "        4. If there are conflicting viewpoints, present them objectively\n",
        "        5. Include relevant statistics, facts, or examples when available\n",
        "        6. Be accurate and cite-worthy\n",
        "        7. Make the answer informative and easy to understand\n",
        "\n",
        "        Provide a comprehensive answer (200-400 words) without any prefixes or formatting markers.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(synthesis_prompt)\n",
        "            synthesized_answer = response.text.strip()\n",
        "\n",
        "            # Determine confidence based on number and quality of sources\n",
        "            confidence = \"high\" if len(search_results) >= 3 else \"medium\" if len(search_results) >= 2 else \"low\"\n",
        "\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': synthesized_answer,\n",
        "                'sources': sources,\n",
        "                'confidence': confidence,\n",
        "                'num_sources': len(search_results)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error synthesizing answer: {e}\")\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': f\"Found {len(search_results)} relevant sources but couldn't synthesize a complete answer. Please check the sources below for information.\",\n",
        "                'sources': sources,\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    def research_dynamic_question(self, question):\n",
        "        \"\"\"\n",
        "        Complete research cycle for a single dynamically generated question\n",
        "\n",
        "        Args:\n",
        "            question (str): Research question\n",
        "\n",
        "        Returns:\n",
        "            dict: Complete research result\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ğŸ“‹ RESEARCHING: {question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Step 1: Search web\n",
        "        search_results = self.search_web_advanced(question)\n",
        "\n",
        "        # Step 2: Synthesize answer\n",
        "        result = self.synthesize_answer(question, search_results)\n",
        "\n",
        "        print(f\"âœ… Research completed - Confidence: {result['confidence']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_user_input(self, user_input, num_questions=5):\n",
        "        \"\"\"\n",
        "        Main method: Process user input dynamically using ReAct pattern\n",
        "\n",
        "        Args:\n",
        "            user_input (str): User's topic or question\n",
        "            num_questions (int): Number of questions to generate\n",
        "\n",
        "        Returns:\n",
        "            dict: Complete research results\n",
        "        \"\"\"\n",
        "        self.original_input = user_input\n",
        "        self.research_results = {}\n",
        "\n",
        "        print(f\"ğŸš€ DYNAMIC ReAct AGENT STARTED\")\n",
        "        print(f\"ğŸ“¥ Processing: '{user_input}'\")\n",
        "        print(f\"ğŸ¯ Will generate {num_questions} dynamic questions\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # PHASE 1: REASONING - Generate dynamic questions\n",
        "        questions = self.generate_dynamic_questions(user_input, num_questions)\n",
        "\n",
        "        if not questions:\n",
        "            return {\"error\": \"Failed to generate research questions\"}\n",
        "\n",
        "        # PHASE 2: ACTING + REASONING - Research each question\n",
        "        for i, question in enumerate(questions, 1):\n",
        "            print(f\"\\nğŸ”„ Processing Question {i}/{len(questions)}\")\n",
        "\n",
        "            result = self.research_dynamic_question(question)\n",
        "            self.research_results[question] = result\n",
        "\n",
        "            # Small delay to be respectful to APIs\n",
        "            time.sleep(1)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ğŸ‰ DYNAMIC RESEARCH COMPLETED!\")\n",
        "        print(f\"ğŸ“Š Processed {len(questions)} questions with {sum(r['num_sources'] for r in self.research_results.values())} total sources\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        return {\n",
        "            'original_input': user_input,\n",
        "            'generated_questions': questions,\n",
        "            'research_results': self.research_results,\n",
        "            'summary': self._generate_summary()\n",
        "        }\n",
        "\n",
        "    def _generate_summary(self):\n",
        "        \"\"\"Generate a summary of all research findings\"\"\"\n",
        "        total_sources = sum(r['num_sources'] for r in self.research_results.values())\n",
        "        high_confidence = sum(1 for r in self.research_results.values() if r['confidence'] == 'high')\n",
        "\n",
        "        return {\n",
        "            'total_questions': len(self.research_results),\n",
        "            'total_sources': total_sources,\n",
        "            'high_confidence_answers': high_confidence,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "def display_research_results(results):\n",
        "    \"\"\"Display research results in a formatted way\"\"\"\n",
        "    if 'error' in results:\n",
        "        print(f\"âŒ {results['error']}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ“‹ RESEARCH REPORT\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ğŸ¯ Original Input: {results['original_input']}\")\n",
        "    print(f\"ğŸ“… Generated: {results['summary']['timestamp']}\")\n",
        "    print(f\"ğŸ“Š Questions: {results['summary']['total_questions']} | Sources: {results['summary']['total_sources']} | High Confidence: {results['summary']['high_confidence_answers']}\")\n",
        "\n",
        "    for i, (question, result) in enumerate(results['research_results'].items(), 1):\n",
        "        print(f\"\\n{'â”€'*60}\")\n",
        "        print(f\"â“ QUESTION {i}: {question}\")\n",
        "        print(f\"{'â”€'*60}\")\n",
        "        print(f\"ğŸ¯ Confidence: {result['confidence'].upper()} | Sources: {result['num_sources']}\")\n",
        "        print(f\"\\nğŸ“ ANSWER:\")\n",
        "        print(result['answer'])\n",
        "\n",
        "        if result['sources']:\n",
        "            print(f\"\\nğŸ“š SOURCES:\")\n",
        "            for j, source in enumerate(result['sources'], 1):\n",
        "                print(f\"   {j}. {source['title']}\")\n",
        "                print(f\"      ğŸ”— {source['url']}\")\n",
        "                if source.get('relevance_score'):\n",
        "                    print(f\"      ğŸ“Š Relevance: {source['relevance_score']}\")\n",
        "                print()\n",
        "\n",
        "# Main Functions for Easy Use\n",
        "\n",
        "def research_anything(user_input, num_questions=5):\n",
        "    \"\"\"\n",
        "    Research any topic or question dynamically\n",
        "\n",
        "    Args:\n",
        "        user_input (str): Your topic, question, or any input\n",
        "        num_questions (int): Number of questions to generate (default: 5)\n",
        "\n",
        "    Example:\n",
        "        research_anything(\"Machine Learning\")\n",
        "        research_anything(\"How do electric cars work?\")\n",
        "        research_anything(\"Tesla company\", 4)\n",
        "    \"\"\"\n",
        "    if GEMINI_API_KEY == \"your_gemini_api_key_here\" or TAVILY_API_KEY == \"your_tavily_api_key_here\":\n",
        "        print(\"âš ï¸  Please set your API keys first!\")\n",
        "        print(\"   - Gemini: https://makersuite.google.com/app/apikey\")\n",
        "        print(\"   - Tavily: https://tavily.com/\")\n",
        "        return None\n",
        "\n",
        "    agent = DynamicReActAgent()\n",
        "    results = agent.process_user_input(user_input, num_questions)\n",
        "    display_research_results(results)\n",
        "    return results\n",
        "\n",
        "def quick_research(topic):\n",
        "    \"\"\"Quick research with 3 questions\"\"\"\n",
        "    return research_anything(topic, 3)\n",
        "\n",
        "def deep_research(topic):\n",
        "    \"\"\"Deep research with 7 questions\"\"\"\n",
        "    return research_anything(topic, 7)\n",
        "\n",
        "# Demo Function\n",
        "def run_demo():\n",
        "    \"\"\"Run a demo with default topics\"\"\"\n",
        "\n",
        "    if GEMINI_API_KEY == \"your_gemini_api_key_here\" or TAVILY_API_KEY == \"your_tavily_api_key_here\":\n",
        "        print(\"âš ï¸  Please replace the API keys with your actual keys!\")\n",
        "        print(\"   - Get Gemini API key: https://makersuite.google.com/app/apikey\")\n",
        "        print(\"   - Get Tavily API key: https://tavily.com/\")\n",
        "        print(\"\\nğŸ”§ After setting keys, you can use:\")\n",
        "        print(\"   â€¢ research_anything('Your topic or question here')\")\n",
        "        print(\"   â€¢ quick_research('Topic')  # 3 questions\")\n",
        "        print(\"   â€¢ deep_research('Topic')   # 7 questions\")\n",
        "        return\n",
        "\n",
        "    print(\"ğŸ¤– DYNAMIC ReAct AGENT DEMO\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Demo topics - the agent will generate questions dynamically\n",
        "    demo_topics = [\n",
        "        \"Renewable Energy\",\n",
        "        \"How does artificial intelligence work?\",\n",
        "        \"SpaceX and space exploration\"\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ¬ Available demo topics:\")\n",
        "    for i, topic in enumerate(demo_topics, 1):\n",
        "        print(f\"   {i}. {topic}\")\n",
        "\n",
        "    print(f\"\\nğŸš€ Running demo with: '{demo_topics[0]}'\")\n",
        "    print(\"(The agent will automatically generate relevant questions)\")\n",
        "\n",
        "    # Run demo\n",
        "    research_anything(demo_topics[0], 4)\n",
        "\n",
        "# Run the demo\n",
        "if __name__ == \"__main__\":\n",
        "    run_demo()\n",
        "\n",
        "# USAGE EXAMPLES - Uncomment to try:\n",
        "\n",
        "# Example 1: Research any topic - agent generates questions automatically\n",
        "# research_anything(\"Climate Change\")\n",
        "\n",
        "# Example 2: Ask a question - agent generates related questions\n",
        "# research_anything(\"How does blockchain technology work?\")\n",
        "\n",
        "# Example 3: Research a company/person - agent generates comprehensive questions\n",
        "# research_anything(\"Elon Musk\", 6)\n",
        "\n",
        "# Example 4: Quick research (3 questions)\n",
        "# quick_research(\"Quantum Computing\")\n",
        "\n",
        "# Example 5: Deep research (7 questions)\n",
        "# deep_research(\"Artificial Intelligence in Healthcare\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ USAGE:\")\n",
        "print(\"research_anything('Your topic here')  # Main function\")\n",
        "print(\"quick_research('Topic')              # 3 questions\")\n",
        "print(\"deep_research('Topic')               # 7 questions\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4c_lZZehMu1w",
        "outputId": "b16581eb-2834-4d49-f50f-7cfca94f66b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– DYNAMIC ReAct AGENT DEMO\n",
            "==================================================\n",
            "ğŸ¬ Available demo topics:\n",
            "   1. Renewable Energy\n",
            "   2. How does artificial intelligence work?\n",
            "   3. SpaceX and space exploration\n",
            "\n",
            "ğŸš€ Running demo with: 'Renewable Energy'\n",
            "(The agent will automatically generate relevant questions)\n",
            "ğŸš€ DYNAMIC ReAct AGENT STARTED\n",
            "ğŸ“¥ Processing: 'Renewable Energy'\n",
            "ğŸ¯ Will generate 4 dynamic questions\n",
            "================================================================================\n",
            "ğŸ§  REASONING PHASE: Analyzing input and generating dynamic questions...\n",
            "ğŸ“ User Input: 'Renewable Energy'\n",
            "âœ… Generated 4 dynamic research questions:\n",
            "   1. What are the current global trends in renewable energy investment and deployment across different technologies (solar, wind, hydro, geothermal, biomass)?\n",
            "   2. What are the major environmental and social impacts of transitioning to a renewable energy-based electricity grid, considering lifecycle assessments and land use changes?\n",
            "   3. How effective are current government policies and incentives in promoting the adoption of renewable energy technologies in various countries, and what are the barriers to wider implementation?\n",
            "   4. What are the projected costs and benefits of achieving a 100% renewable energy future by 2050, considering technological advancements, energy storage solutions, and grid modernization?\n",
            "\n",
            "ğŸ”„ Processing Question 1/4\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the current global trends in renewable energy investment and deployment across different technologies (solar, wind, hydro, geothermal, biomass)?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the current global trends in renewable energy investment and deployment across different technologies (solar, wind, hydro, geothermal, biomass)?\n",
            "âœ… Found 3 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the current global trends in renewable en...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 2/4\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the major environmental and social impacts of transitioning to a renewable energy-based electricity grid, considering lifecycle assessments and land use changes?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the major environmental and social impacts of transitioning to a renewable energy-based electricity grid, considering lifecycle assessments and land use changes?\n",
            "âœ… Found 4 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the major environmental and social impact...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 3/4\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: How effective are current government policies and incentives in promoting the adoption of renewable energy technologies in various countries, and what are the barriers to wider implementation?\n",
            "============================================================\n",
            "ğŸ” Searching: How effective are current government policies and incentives in promoting the adoption of renewable energy technologies in various countries, and what are the barriers to wider implementation?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'How effective are current government policies and ...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 4/4\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the projected costs and benefits of achieving a 100% renewable energy future by 2050, considering technological advancements, energy storage solutions, and grid modernization?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the projected costs and benefits of achieving a 100% renewable energy future by 2050, considering technological advancements, energy storage solutions, and grid modernization?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the projected costs and benefits of achie...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ DYNAMIC RESEARCH COMPLETED!\n",
            "ğŸ“Š Processed 4 questions with 17 total sources\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ RESEARCH REPORT\n",
            "================================================================================\n",
            "ğŸ¯ Original Input: Renewable Energy\n",
            "ğŸ“… Generated: 2025-06-13 07:49:53\n",
            "ğŸ“Š Questions: 4 | Sources: 17 | High Confidence: 4\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 1: What are the current global trends in renewable energy investment and deployment across different technologies (solar, wind, hydro, geothermal, biomass)?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 3\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Global renewable energy investment and deployment in 2023 reached record levels, totaling USD 622.5 billion.  This represents an 8.1% increase from 2022 and is notable considering challenges like high interest rates and increased raw material costs.  Solar photovoltaic (PV) and wind power overwhelmingly dominated investment, mirroring trends from the previous year.  Solar PV accounted for a substantial 63% of the total investment, reaching USD 392.7 billion, while wind power secured 35%.  While solar PV investment saw growth of 12.5% compared to 2022, this represents a significant slowdown from the growth rates observed in 2021 (39%) and 2022 (44%).  A significant portion of solar PV investment, nearly half (47.4%), was concentrated in China.\n",
            "\n",
            "The sources do not provide detailed breakdown of investment and deployment for hydro, geothermal, and biomass energy.  However, the dominance of solar and wind clearly indicates these technologies are the primary recipients of investment, reflecting global efforts toward decarbonization.  The record investment in 2023 is attributed to several factors including the global energy crisis, post-pandemic recovery, and a strong alignment between climate change policy ambitions, energy security goals, and industrial strategies. This convergence of factors demonstrates a powerful impetus driving the global transition to renewable energy sources.  Future trends will likely depend on continued policy support, technological advancements, and the ongoing global energy landscape.  While the provided data focuses on 2023, it highlights a strong trajectory for renewable energy investment, with solar and wind technologies leading the charge.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Renewables in Energy Supply: Global Trends | Investment ... - REN21\n",
            "      ğŸ”— https://www.ren21.net/gsr-2024/modules/energy_supply/01_global_trends/03_investment_and_finance/\n",
            "      ğŸ“Š Relevance: 0.82761955\n",
            "\n",
            "   3. RENEWABLE ENERGY IN ENERGY SUPPLY | Global Trends\n",
            "      ğŸ”— https://www.ren21.net/gsr-2024/modules/energy_supply/01_global_trends/\n",
            "      ğŸ“Š Relevance: 0.7882741\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 2: What are the major environmental and social impacts of transitioning to a renewable energy-based electricity grid, considering lifecycle assessments and land use changes?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 4\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Transitioning to a renewable energy-based electricity grid offers significant environmental benefits, primarily through the reduction of greenhouse gas emissions, as noted by Source 1. However, this shift also presents considerable environmental and social challenges that require careful consideration.\n",
            "\n",
            "Lifecycle assessments reveal that while renewable energy sources significantly reduce carbon emissions compared to fossil fuels, they are not entirely without environmental impact. Source 2 highlights that various byproducts, including gases, are released throughout the lifecycle of renewable energy production, negatively impacting air and soil quality.  These impacts vary significantly depending on the specific technology.  For instance, Source 3 provides a breakdown of the impacts of different renewable energy technologies, indicating that while wind and solar technologies have mainly minor impacts like noise and visual effects (minor bird strikes for wind), hydropower can lead to significant displacement of communities and damage to river ecosystems (minor-major magnitude).  Geothermal energy presents a risk of minor seismic activity and pollution.  Photovoltaic systems pose a minor-to-major risk from toxins in their production.  The magnitude of these impacts is not uniform and depends heavily on the specific project and location.\n",
            "\n",
            "Land use changes represent another significant environmental impact.  The large-scale deployment of solar and wind farms requires substantial land areas, potentially impacting biodiversity and ecosystems. While the exact figures aren't provided in these sources, the need for large-scale deployment is implicitly acknowledged.  Source 4 further emphasizes the importance of considering the environmental effects of pollutant emissions and resource consumption, underscoring the need for a comprehensive lifecycle assessment approach.\n",
            "\n",
            "Social impacts are equally important and diverse.  Source 3's table illustrates that some technologies, particularly hydropower, can have major social impacts through displacement of communities, while others, like wind and solar, predominantly generate minor impacts like noise and visual concerns.  Source 1 notes that the social impacts vary widely depending on the specific technology and implementation.  The increase in renewable energy adoption (7% globally since the Paris Agreement according to Source 2) suggests a significant shift, but the associated social and environmental trade-offs necessitate careful planning and mitigation strategies.  Finally, while Source 3 provides efficiency numbers for different energy generation methods, it doesn't directly link them to the environmental or social impacts discussed, highlighting a need for further research integrating these factors for a more complete picture.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Eco-environmental, and social impacts of producing electricity with ...\n",
            "      ğŸ”— https://www.sciencedirect.com/science/article/pii/S0360544225007819\n",
            "      ğŸ“Š Relevance: 0.68191797\n",
            "\n",
            "   3. Social, Economic, and Environmental Impacts of Renewable Energy ...\n",
            "      ğŸ”— https://www.intechopen.com/chapters/70874\n",
            "      ğŸ“Š Relevance: 0.6538572\n",
            "\n",
            "   4. Life Cycle Assessment in Renewable Energy: Solar and Wind ...\n",
            "      ğŸ”— https://www.mdpi.com/2076-3298/11/7/147\n",
            "      ğŸ“Š Relevance: 0.6041426\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 3: How effective are current government policies and incentives in promoting the adoption of renewable energy technologies in various countries, and what are the barriers to wider implementation?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Government policies and incentives show varying effectiveness in promoting renewable energy adoption across countries.  While some nations, like Denmark, have achieved significant success through consistent policies such as favorable feed-in tariffs and grid integration focused on wind energy (Source 3), others face challenges.  Source 1 summarizes the situation succinctly, stating that while some countries see effective promotion, barriers like high costs and political instability hamper wider implementation.\n",
            "\n",
            "A key element highlighted by Source 2 is the diversity of policy approaches. Germany's reliance on feed-in tariffs, Denmark's focus on community ownership of wind energy, and Spain's (partially described) strategy demonstrate that no single solution works universally.  However, Source 2 also points out the vulnerability of renewable energy incentives to political shifts.  Policy uncertainty, a major deterrent to long-term investments in renewable projects, undermines their effectiveness.\n",
            "\n",
            "Financial incentives, including tax credits, grants, and feed-in tariffs, are crucial (Source 3).  The US Investment Tax Credit (ITC) for solar energy is cited as a success story.  Conversely, Source 4 and Source 5 emphasize the high initial costs of renewable energy infrastructure as a significant barrier.  Source 5 further argues that inconsistent government policies, particularly conflicting state and federal regulations (as seen in the US), hinder market development.  The example of Denmark's success, conversely, emphasizes the importance of coordinated national policies with long durations to foster market growth (Source 5).  Inconsistent government incentives and market barriers further reduce widespread adoption, according to Source 4.\n",
            "\n",
            "In summary, the effectiveness of current government policies is mixed.  Successful strategies involve consistent, long-term policies that integrate financial incentives (like feed-in tariffs and tax credits), address grid integration issues, and possibly incorporate community ownership models.  However, high initial costs, political instability, and inconsistent policy frameworks across different levels of government (as seen in the US example) present significant hurdles to broader renewable energy adoption.  Comparative policy analyses, as suggested by Source 2, remain essential for understanding the successes and failures of diverse approaches.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. How Do Governments Incentivize Renewable Adoption? â†’ Question\n",
            "      ğŸ”— https://energy.sustainability-directory.com/question/how-do-governments-incentivize-renewable-adoption/\n",
            "      ğŸ“Š Relevance: 0.85421735\n",
            "\n",
            "   3. Promoting Renewable Energy Incentives and Regulatory Frameworks\n",
            "      ğŸ”— https://www.cyis.org/post/promoting-renewable-energy-incentives-and-regulatory-frameworks\n",
            "      ğŸ“Š Relevance: 0.843393\n",
            "\n",
            "   4. (PDF) Barriers to Renewable Energy Technologies Development\n",
            "      ğŸ”— https://www.researchgate.net/publication/348936339_Barriers_to_Renewable_Energy_Technologies_Development\n",
            "      ğŸ“Š Relevance: 0.80844593\n",
            "\n",
            "   5. [PDF] Barriers to Adoption of Renewable Energy Technology\n",
            "      ğŸ”— https://irps.illinoisstate.edu/downloads/research/student/IRPSDonerWorkingPaper050707.pdf\n",
            "      ğŸ“Š Relevance: 0.80771893\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 4: What are the projected costs and benefits of achieving a 100% renewable energy future by 2050, considering technological advancements, energy storage solutions, and grid modernization?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Achieving a 100% renewable energy future by 2050 presents significant costs and benefits.  A major cost factor is grid modernization, estimated at around $21 trillion by the World Economic Forum (Source 3). This investment is crucial for efficiently integrating renewable energy sources and enhancing grid resilience (Source 3).  Source 2, from NREL, provides a more nuanced perspective on the cost of transitioning to 100% renewable generation.  While a base-case scenario projects levelized costs of $30 per megawatt-hour (MWh) with 57% renewable energy by 2050,  reaching 100% renewables increases costs by less than $10/MWh (29% increase) under the modeled conditions.  However, this cost increase is non-linear, with the final percentage points proving most expensive.  Capital costs for renewable technologies and energy storage are the largest contributors to the overall system cost (Source 2), highlighting the importance of technological advancements in reducing these expenses.\n",
            "\n",
            "The speed of the transition also significantly impacts cost and emissions reduction.  Faster transitions are more expensive but lead to greater cumulative emissions reductions (Source 2).  Technological advancements and efficient energy storage solutions are key to mitigating costs. Energy storage, crucial for managing the intermittency of renewable sources, is projected to meet a substantial portion of flexibility needs by 2050 in both advanced and emerging economies, according to the International Energy Agency (Source 4).  Source 1 summarizes the overall cost as approximately $21 trillion for grid modernization, but emphasizes that emission reductions and economic benefits will offset this expense.  \n",
            "\n",
            "While the $21 trillion figure for grid modernization (Sources 1, 3) is significant, the long-term economic benefits are substantial.  Avoiding trillions of dollars in annual economic losses due to climate change and lost growth potential is a substantial counterbalance to the upfront investment (Source 3).  Furthermore, improved energy access for disadvantaged communities is a potential social benefit (Source 3).  A limitation of available research is the lack of detailed analysis on the role of smart grids and energy storage in achieving the transition (Source 5). Therefore, further research is needed for a more comprehensive cost-benefit analysis.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Quantifying the Costs and Emissions Benefits of a 100% Renewable ...\n",
            "      ğŸ”— https://www.nrel.gov/news/detail/program/2021/the-challenge-of-the-last-few-percent-quantifying-the-costs-and-emissions-benefits-of-100-renewables\n",
            "      ğŸ“Š Relevance: 0.776708\n",
            "\n",
            "   3. Grid Modernization: Unlocking the Full Potential of Renewable Energy\n",
            "      ğŸ”— https://www.energycentral.com/energy-management/post/grid-modernization-unlocking-the-full-potential-of-renewable-energy-gtI1OmMaNYlmYTj\n",
            "      ğŸ“Š Relevance: 0.6708892\n",
            "\n",
            "   4. Grid expansion and modernization | Deloitte Insights\n",
            "      ğŸ”— https://www2.deloitte.com/us/en/insights/industry/power-and-utilities/grid-modernization-and-expansion-critical-for-clean-energy-future.html\n",
            "      ğŸ“Š Relevance: 0.5826579\n",
            "\n",
            "   5. [PDF] 100% RENEWABLE ENERGY BY 2050 - image\n",
            "      ğŸ”— http://awsassets.panda.org/downloads/the_energy_report_lowres_111110.pdf\n",
            "      ğŸ“Š Relevance: 0.5324014\n",
            "\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ USAGE:\n",
            "research_anything('Your topic here')  # Main function\n",
            "quick_research('Topic')              # 3 questions\n",
            "deep_research('Topic')               # 7 questions\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "research_anything('LLM integration')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgLu4E9pQRLC",
        "outputId": "6a2d34eb-6738-4a31-eaec-12f724d2c380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ DYNAMIC ReAct AGENT STARTED\n",
            "ğŸ“¥ Processing: 'LLM integration'\n",
            "ğŸ¯ Will generate 5 dynamic questions\n",
            "================================================================================\n",
            "ğŸ§  REASONING PHASE: Analyzing input and generating dynamic questions...\n",
            "ğŸ“ User Input: 'LLM integration'\n",
            "âœ… Generated 5 dynamic research questions:\n",
            "   1. What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?\n",
            "   2. What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?\n",
            "   3. How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?\n",
            "   4. What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?\n",
            "   5. What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?\n",
            "\n",
            "ğŸ”„ Processing Question 1/5\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the current best practices for integratin...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 2/5\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the key challenges and limitations in dep...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 3/5\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?\n",
            "============================================================\n",
            "ğŸ” Searching: How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?\n",
            "âœ… Found 3 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'How are different industries (e.g., healthcare, fi...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 4/5\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the ethical considerations and potential ...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "ğŸ”„ Processing Question 5/5\n",
            "\n",
            "============================================================\n",
            "ğŸ“‹ RESEARCHING: What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?\n",
            "============================================================\n",
            "ğŸ” Searching: What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?\n",
            "âœ… Found 5 sources\n",
            "ğŸ§  REASONING PHASE: Synthesizing answer for 'What are the latest research trends and advancemen...'\n",
            "âœ… Research completed - Confidence: high\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ DYNAMIC RESEARCH COMPLETED!\n",
            "ğŸ“Š Processed 5 questions with 23 total sources\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ RESEARCH REPORT\n",
            "================================================================================\n",
            "ğŸ¯ Original Input: LLM integration\n",
            "ğŸ“… Generated: 2025-06-13 07:52:41\n",
            "ğŸ“Š Questions: 5 | Sources: 23 | High Confidence: 5\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 1: What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Integrating Large Language Models (LLMs) into existing software applications requires a strategic approach encompassing several key best practices.  Firstly, the choice of integration method is crucial.  Sources 4 and 3 highlight two primary options: utilizing pre-built Software Development Kits (SDKs) and libraries offered by LLM providers (like those for Python, JavaScript, and Java), or developing custom code. SDKs simplify integration, while custom code allows for greater control and adaptation to specific needs.  Experimenting with API calls using tools like Postman or curl before full-scale integration is recommended (Source 4).\n",
            "\n",
            "Secondly, version pinning is essential for maintaining application stability. Source 5 strongly advises specifying the exact LLM model version (e.g., \"gemini-1.5-pro-001\" instead of just \"gemini-1.5-pro\") to prevent unexpected behavior from updates.  This ensures consistent functionality and avoids potential disruptions caused by unforeseen changes in model performance or output.\n",
            "\n",
            "Thirdly, continuous improvement and updates are vital. Sources 1 and 2 emphasize the importance of incorporating feedback loops to refine the model's performance based on user interaction.  Regular maintenance and updates are also necessary to leverage advancements in LLM technology and address security vulnerabilities.  Source 5 reinforces this by highlighting the rapid evolution of the LLM landscape and the need for applications to adapt continuously.\n",
            "\n",
            "Furthermore,  Source 2 underscores the importance of proactive planning. This includes addressing potential challenges such as data privacy concerns (requiring adherence to relevant regulations and robust security measures), assessing team skill levels and providing necessary training, and establishing clear expectations for the integration process.  Careful consideration of data handling and security is paramount due to LLMs often processing sensitive information.\n",
            "\n",
            "In summary, successful LLM integration involves a combination of selecting appropriate integration methods (SDKs or custom code), meticulous version control to ensure stability, continuous monitoring and improvement through feedback loops and regular updates, and thorough preparation to address potential challenges, including data privacy and team skill gaps.  Following these best practices maximizes the benefits of LLM integration while mitigating potential risks.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Integrating Large Language Models with Existing IT Infrastructure\n",
            "      ğŸ”— https://www.algomox.com/resources/blog/how_to_integrate_llms_with_it_infrastructure.html\n",
            "      ğŸ“Š Relevance: 0.82425016\n",
            "\n",
            "   3. Integrating Language Models into Existing Software Systems\n",
            "      ğŸ”— https://www.kdnuggets.com/integrating-language-models-into-existing-software-systems\n",
            "      ğŸ“Š Relevance: 0.7935067\n",
            "\n",
            "   4. Mastering LLM Integration: 6 Steps Every CTO Should Follow\n",
            "      ğŸ”— https://hatchworks.com/blog/gen-ai/llm-integration-guide/\n",
            "      ğŸ“Š Relevance: 0.78512776\n",
            "\n",
            "   5. Some advice and good practices when integrating an LLM in your ...\n",
            "      ğŸ”— https://glaforge.dev/posts/2024/09/23/some-good-practices-when-integrating-an-llm-in-your-application/\n",
            "      ğŸ“Š Relevance: 0.7402687\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 2: What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Deploying LLM-integrated applications presents significant challenges in achieving both scalability and cost-effectiveness.  High computational demands and resource strain are central issues, as noted by Tavily AI (Source 1).  This strain manifests in several ways.  Scalability issues arise from the difficulty of managing large workloads and unpredictable traffic spikes (Source 2, Source 4).  LLM tools need to perform well under high demand, otherwise user frustration and system unresponsiveness result (Source 2).  Coralogix (Source 4) emphasizes that the need for distributed computing to handle vast datasets and high-throughput inference requires sophisticated and optimized infrastructure.  This infrastructure complexity contributes directly to cost concerns.\n",
            "\n",
            "Cost management is a constant balancing act between infrastructure efficiency and operational expenses (Source 1, Source 5).  Deploying and maintaining LLMs in production environments involves substantial ongoing costs (Source 4).  The computational resources required by LLMs often strain existing infrastructure and drive up operational expenses (Source 5).  Balancing latency and inference throughput with cost-effective infrastructure is a key challenge for organizations deploying LLMs in real-world applications (Source 5).  Solutions like Wallaroo.AI aim to address this by offering platforms that optimize custom LLM deployment at scale, leveraging features such as autoscaling and real-time monitoring to reduce costs (Source 5).\n",
            "\n",
            "Beyond infrastructure, other deployment challenges include compatibility with existing systems and APIs (Source 2),  memory limitations and latency issues (Source 3), and security concerns (Source 3).  Furthermore, the \"hallucinations\" or inaccuracies inherent in some LLMs, along with ethical considerations and the need for continuous model updates, present additional complexities (Source 2).  Addressing these challenges requires a multifaceted approach involving optimized infrastructure, efficient algorithms, robust security measures, and careful consideration of the ongoing operational costs associated with maintaining and updating LLM-powered applications.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. 6 biggest LLM challenges and possible solutions - nexos.ai\n",
            "      ğŸ”— https://nexos.ai/blog/llm-challenges/\n",
            "      ğŸ“Š Relevance: 0.81630427\n",
            "\n",
            "   3. Navigating Deployment of LLMs to Cloud Servers - Walturn\n",
            "      ğŸ”— https://www.walturn.com/insights/navigating-deployment-of-llms-to-cloud-servers\n",
            "      ğŸ“Š Relevance: 0.80625874\n",
            "\n",
            "   4. Top Challenges in Building Enterprise LLM Applications - Coralogix\n",
            "      ğŸ”— https://coralogix.com/ai-blog/top-challenges-in-building-enterprise-llm-applications/\n",
            "      ğŸ“Š Relevance: 0.7931224\n",
            "\n",
            "   5. Cost-Effective Deployment of Large LLMs: Overcoming Infrastructure ...\n",
            "      ğŸ”— https://wallaroo.ai/cost-effective-deployment-of-large-llms-overcoming-infrastructure-constraints/\n",
            "      ğŸ“Š Relevance: 0.78963995\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 3: How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 3\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Large language models (LLMs) are rapidly transforming various industries by enhancing efficiency and creating novel functionalities.  Across healthcare, finance, and education, LLMs are automating tasks, improving decision-making, and personalizing service delivery.  In healthcare, LLMs can automate administrative tasks, analyze medical records to assist with diagnosis, and personalize patient care plans, ultimately improving workflows and reducing errors (Source 1).  Similarly, in finance, LLMs can streamline processes like fraud detection, risk assessment, and customer service, leading to improved efficiency and reduced operational costs (Source 1, Source 2).  The education sector benefits from LLMs through personalized learning experiences, automated grading, and the creation of intelligent tutoring systems (Source 1, Source 2).\n",
            "\n",
            "Source 2 highlights the potential of integrating LLMs with federated learning (FedLLMs) to address challenges related to sensitive data in these industries.  This approach allows for the training and application of powerful models without relying on centralized data storage, thus enhancing security and scalability while potentially promoting more equitable access to LLM technology.  The integration of LLMs and federated learning is presented as a transformative technology, particularly for industries handling sensitive data (Source 2).\n",
            "\n",
            "Furthermore, platforms like LeewayHertz's ZBrain demonstrate the practical application of LLMs in creating custom applications tailored to specific business needs (Source 3).  ZBrain leverages various advanced language models (GPT, Llama, PaLM, etc.) to process diverse data types and build custom \"Flows,\" complex business logic forming the foundation of LLM-based applications.  This approach not only enhances precision but also minimizes errors and addresses challenges such as inconsistent supplier performance and prolonged decision-making (Source 3).  These customized applications are crucial for optimizing operational workflows and improving customer service across diverse industries.  While specific statistics or quantifiable improvements are not provided in the sources, the overall consensus points to significant potential for efficiency gains and functional enhancements through LLM integration in healthcare, finance, and education.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Review Integration of large language models and federated learning\n",
            "      ğŸ”— https://www.sciencedirect.com/science/article/pii/S2666389924002708\n",
            "      ğŸ“Š Relevance: 0.73505294\n",
            "\n",
            "   3. AI Use Cases & Applications Across Major industries - LeewayHertz\n",
            "      ğŸ”— https://www.leewayhertz.com/ai-use-cases-and-applications/\n",
            "      ğŸ“Š Relevance: 0.47022632\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 4: What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Deploying Large Language Models (LLMs) presents significant ethical considerations and potential biases that demand careful mitigation.  A primary concern is bias, stemming from the vast datasets used for training. These datasets, often sourced from the internet, can reflect and amplify existing societal biases, leading to outputs that perpetuate stereotypes or discriminate against certain groups (Source 2).  This bias can manifest in various ways, impacting different applications differently. For example, in healthcare, biased LLMs could negatively influence the quality of care provided to specific patient groups (Source 4).  Mitigation strategies include employing diverse and representative datasets during training (Source 3), implementing techniques for identifying and mitigating bias during model development (Source 2), and conducting regular audits of LLM outputs to ensure fairness and ethical standards (Source 3).\n",
            "\n",
            "Privacy is another critical concern.  The use of personal data in training and application necessitates robust data protection measures to prevent unauthorized access or misuse (Source 1).  Transparency is crucial for building user trust and enabling informed usage.  Developers should disclose the training data, processes, and potential biases (Source 3), fostering accountability.  Without transparency, it becomes difficult to assess and address potential harms. Source 2 emphasizes the need for transparency regarding model capabilities, limitations, and training data.\n",
            "\n",
            "Accountability is also paramount.  Determining responsibility for LLM outputs and their consequences is a complex challenge.  Source 4 discusses the challenges of clinical accountability when using LLMs in medical practice.  Establishing independent ethics review boards to assess LLM development plans before deployment could help mitigate risks and ensure alignment with societal values (Source 3). The potential for misuse of LLMs, such as generating misinformation or harmful content, necessitates careful consideration of safeguards and responsible deployment practices.  While LLMs offer potential benefits across various applications, these ethical considerations must be proactively addressed to ensure their responsible and equitable implementation.  Addressing bias, protecting privacy, ensuring transparency, and establishing accountability mechanisms are vital steps in mitigating the risks associated with LLM deployment.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. Ethical Considerations in LLM Development and Deployment\n",
            "      ğŸ”— https://dev.to/nareshnishad/ethical-considerations-in-llm-development-and-deployment-3j6n\n",
            "      ğŸ“Š Relevance: 0.86954874\n",
            "\n",
            "   3. What are Ethics and Bias in LLMs? - AI Agent Builder\n",
            "      ğŸ”— https://www.appypieagents.ai/blog/ethics-and-bias-in-llms\n",
            "      ğŸ“Š Relevance: 0.8660535\n",
            "\n",
            "   4. Implications of Large Language Models for Clinical Practice: Ethical ...\n",
            "      ğŸ”— https://pmc.ncbi.nlm.nih.gov/articles/PMC11609490/\n",
            "      ğŸ“Š Relevance: 0.8098936\n",
            "\n",
            "   5. Ethical Considerations and Bias Mitigation in Large Language ...\n",
            "      ğŸ”— https://www.researchgate.net/publication/387295166_Ethical_Considerations_and_Bias_Mitigation_in_Large_Language_Models_AUTHOR\n",
            "      ğŸ“Š Relevance: 0.80442166\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION 5: What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ¯ Confidence: HIGH | Sources: 5\n",
            "\n",
            "ğŸ“ ANSWER:\n",
            "Recent research in LLM integration centers on enhancing performance and accessibility through improved prompt engineering and fine-tuning techniques.  Prompt engineering is evolving beyond simple input phrasing.  Source 2 highlights \"iterative refinement,\" where prompts are repeatedly tested and adjusted, and \"context layering,\" where crucial information is embedded within the prompt to direct the LLM's response.  These methods, applicable to diverse tasks from creative writing to code assistance, are increasingly crucial for optimizing LLM outputs across various industries.\n",
            "\n",
            "Fine-tuning methods are also experiencing significant advancements.  Sources 1 and 3 identify \"prompt tuning\" as a key trend, focusing on refining prompts to improve model responses. This approach often complements traditional fine-tuning, offering a balance between performance gains and computational efficiency.  The automation of fine-tuning through AutoML (Sources 1 and 3) is accelerating the process and broadening access for researchers and developers.  AutoML automates tasks such as hyperparameter selection and optimization.\n",
            "\n",
            "Zero-shot and few-shot learning (Sources 3 and 4) are emerging as significant LLM fine-tuning techniques, enabling models to handle tasks with minimal or no task-specific data. While promising, challenges remain, including potential overfitting and limitations in evaluation methodologies.  Source 4 discusses the application of few-shot prompt engineering and fine-tuning in Amazon Bedrock, indicating its practical implementation in real-world applications.\n",
            "\n",
            "The trend towards task specialization and fine-tuning is also prominent, exemplified by models like Med-PaLM 2 and Radiology-Llama2 (Source 5).  These models, fine-tuned with domain-specific data, demonstrate enhanced accuracy and context relevance within specialized fields like healthcare and radiology.  This targeted fine-tuning represents a move away from general-purpose LLMs towards models optimized for specific tasks and industries, showcasing the growing sophistication of LLM integration techniques.  Overall, research suggests a convergence of improved prompt engineering methodologies with increasingly efficient and specialized fine-tuning approaches, driving the expansion and utility of LLMs across diverse domains.\n",
            "\n",
            "ğŸ“š SOURCES:\n",
            "   1. Direct Answer Summary\n",
            "      ğŸ”— Tavily AI Summary\n",
            "      ğŸ“Š Relevance: 1.0\n",
            "\n",
            "   2. The Future of LLM Prompt Engineering: Trends and Innovation\n",
            "      ğŸ”— https://mindy-support.com/news-post/the-future-of-llm-prompt-engineering-trends-and-innovation/\n",
            "      ğŸ“Š Relevance: 0.7574541\n",
            "\n",
            "   3. 18 Artificial Intelligence LLM Trends in 2025 | AI Bistrot - Medium\n",
            "      ğŸ”— https://medium.com/data-bistrot/15-artificial-intelligence-llm-trends-in-2024-618a058c9fdf\n",
            "      ğŸ“Š Relevance: 0.6922474\n",
            "\n",
            "   4. Few-shot prompt engineering and fine-tuning for LLMs in Amazon ...\n",
            "      ğŸ”— https://aws.amazon.com/blogs/machine-learning/few-shot-prompt-engineering-and-fine-tuning-for-llms-in-amazon-bedrock/\n",
            "      ğŸ“Š Relevance: 0.6847074\n",
            "\n",
            "   5. The Future of Large Language Models in 2025 - Research AIMultiple\n",
            "      ğŸ”— https://research.aimultiple.com/future-of-large-language-models/\n",
            "      ğŸ“Š Relevance: 0.64880073\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_input': 'LLM integration',\n",
              " 'generated_questions': ['What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?',\n",
              "  'What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?',\n",
              "  'How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?',\n",
              "  'What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?',\n",
              "  'What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?'],\n",
              " 'research_results': {'What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?': {'question': 'What are the current best practices for integrating Large Language Models (LLMs) into existing software applications?',\n",
              "   'answer': 'Integrating Large Language Models (LLMs) into existing software applications requires a strategic approach encompassing several key best practices.  Firstly, the choice of integration method is crucial.  Sources 4 and 3 highlight two primary options: utilizing pre-built Software Development Kits (SDKs) and libraries offered by LLM providers (like those for Python, JavaScript, and Java), or developing custom code. SDKs simplify integration, while custom code allows for greater control and adaptation to specific needs.  Experimenting with API calls using tools like Postman or curl before full-scale integration is recommended (Source 4).\\n\\nSecondly, version pinning is essential for maintaining application stability. Source 5 strongly advises specifying the exact LLM model version (e.g., \"gemini-1.5-pro-001\" instead of just \"gemini-1.5-pro\") to prevent unexpected behavior from updates.  This ensures consistent functionality and avoids potential disruptions caused by unforeseen changes in model performance or output.\\n\\nThirdly, continuous improvement and updates are vital. Sources 1 and 2 emphasize the importance of incorporating feedback loops to refine the model\\'s performance based on user interaction.  Regular maintenance and updates are also necessary to leverage advancements in LLM technology and address security vulnerabilities.  Source 5 reinforces this by highlighting the rapid evolution of the LLM landscape and the need for applications to adapt continuously.\\n\\nFurthermore,  Source 2 underscores the importance of proactive planning. This includes addressing potential challenges such as data privacy concerns (requiring adherence to relevant regulations and robust security measures), assessing team skill levels and providing necessary training, and establishing clear expectations for the integration process.  Careful consideration of data handling and security is paramount due to LLMs often processing sensitive information.\\n\\nIn summary, successful LLM integration involves a combination of selecting appropriate integration methods (SDKs or custom code), meticulous version control to ensure stability, continuous monitoring and improvement through feedback loops and regular updates, and thorough preparation to address potential challenges, including data privacy and team skill gaps.  Following these best practices maximizes the benefits of LLM integration while mitigating potential risks.',\n",
              "   'sources': [{'title': 'Direct Answer Summary',\n",
              "     'url': 'Tavily AI Summary',\n",
              "     'content': 'Best practices for integrating LLMs include using SDKs or custom code, pinning specific model versions, and maintaining continuous updates and feedback loops.',\n",
              "     'relevance_score': 1.0},\n",
              "    {'title': 'Integrating Large Language Models with Existing IT Infrastructure',\n",
              "     'url': 'https://www.algomox.com/resources/blog/how_to_integrate_llms_with_it_infrastructure.html',\n",
              "     'content': 'effective. Implementing feedback loops, where user feedback is used to refine and improve the model, is crucial for ongoing optimization. Regular updates and maintenance are also necessary to keep the model aligned with the latest advancements and security protocols. Lastly, fostering a culture of i...',\n",
              "     'relevance_score': 0.82425016},\n",
              "    {'title': 'Integrating Language Models into Existing Software Systems',\n",
              "     'url': 'https://www.kdnuggets.com/integrating-language-models-into-existing-software-systems',\n",
              "     'content': 'Ways to Integrate LLMs in an Existing Software System Â· 1. Choosing the right LLM or API Provider Â· 2. Selecting the Desired Integration Mechanism.',\n",
              "     'relevance_score': 0.7935067},\n",
              "    {'title': 'Mastering LLM Integration: 6 Steps Every CTO Should Follow',\n",
              "     'url': 'https://hatchworks.com/blog/gen-ai/llm-integration-guide/',\n",
              "     'content': '#### How to Connect the LLM with Existing Systems\\n\\nTo connect an LLM to your existing systems, you usually have two options: using pre-built SDKs and libraries or writing custom code.\\n\\nWhich one you choose depends on your technical stack and the level of customization you need.\\n\\n**Using SDKs or Libr...',\n",
              "     'relevance_score': 0.78512776},\n",
              "    {'title': 'Some advice and good practices when integrating an LLM in your ...',\n",
              "     'url': 'https://glaforge.dev/posts/2024/09/23/some-good-practices-when-integrating-an-llm-in-your-application/',\n",
              "     'content': 'To avoid these unexpected changes, the key principle is to **pin the specific version of the LLM model** that you use for your application. For example, when using Gemini 1.5 Pro, if you use the version `gemini-1.5-pro`, youâ€™re actually using the latest version of the model. Currently, itâ€™s `gemini-...',\n",
              "     'relevance_score': 0.7402687}],\n",
              "   'confidence': 'high',\n",
              "   'num_sources': 5},\n",
              "  'What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?': {'question': 'What are the key challenges and limitations in deploying LLM-integrated applications, focusing on scalability and cost-effectiveness?',\n",
              "   'answer': 'Deploying LLM-integrated applications presents significant challenges in achieving both scalability and cost-effectiveness.  High computational demands and resource strain are central issues, as noted by Tavily AI (Source 1).  This strain manifests in several ways.  Scalability issues arise from the difficulty of managing large workloads and unpredictable traffic spikes (Source 2, Source 4).  LLM tools need to perform well under high demand, otherwise user frustration and system unresponsiveness result (Source 2).  Coralogix (Source 4) emphasizes that the need for distributed computing to handle vast datasets and high-throughput inference requires sophisticated and optimized infrastructure.  This infrastructure complexity contributes directly to cost concerns.\\n\\nCost management is a constant balancing act between infrastructure efficiency and operational expenses (Source 1, Source 5).  Deploying and maintaining LLMs in production environments involves substantial ongoing costs (Source 4).  The computational resources required by LLMs often strain existing infrastructure and drive up operational expenses (Source 5).  Balancing latency and inference throughput with cost-effective infrastructure is a key challenge for organizations deploying LLMs in real-world applications (Source 5).  Solutions like Wallaroo.AI aim to address this by offering platforms that optimize custom LLM deployment at scale, leveraging features such as autoscaling and real-time monitoring to reduce costs (Source 5).\\n\\nBeyond infrastructure, other deployment challenges include compatibility with existing systems and APIs (Source 2),  memory limitations and latency issues (Source 3), and security concerns (Source 3).  Furthermore, the \"hallucinations\" or inaccuracies inherent in some LLMs, along with ethical considerations and the need for continuous model updates, present additional complexities (Source 2).  Addressing these challenges requires a multifaceted approach involving optimized infrastructure, efficient algorithms, robust security measures, and careful consideration of the ongoing operational costs associated with maintaining and updating LLM-powered applications.',\n",
              "   'sources': [{'title': 'Direct Answer Summary',\n",
              "     'url': 'Tavily AI Summary',\n",
              "     'content': 'Deploying LLM-integrated applications faces scalability and cost-effectiveness challenges due to high computational demands and resource strain. Scalability issues arise from managing large workloads, while cost management involves balancing infrastructure efficiency and operational expenses.',\n",
              "     'relevance_score': 1.0},\n",
              "    {'title': '6 biggest LLM challenges and possible solutions - nexos.ai',\n",
              "     'url': 'https://nexos.ai/blog/llm-challenges/',\n",
              "     'content': '### Challenges with LLM tools\\n\\nWhen integrating an LLM or an LLM-based tool into existing workflows, companies may face difficulties with compatibility, scalability, maintenance, and security. [...] *   **Tool compatibility.** It can be challenging and may require adjustments to make your LLM tool c...',\n",
              "     'relevance_score': 0.81630427},\n",
              "    {'title': 'Navigating Deployment of LLMs to Cloud Servers - Walturn',\n",
              "     'url': 'https://www.walturn.com/insights/navigating-deployment-of-llms-to-cloud-servers',\n",
              "     'content': 'Challenges in Deployment: High computational demands, memory limitations, latency, cost management, security, and scalability. Open-Source LLMs',\n",
              "     'relevance_score': 0.80625874},\n",
              "    {'title': 'Top Challenges in Building Enterprise LLM Applications - Coralogix',\n",
              "     'url': 'https://coralogix.com/ai-blog/top-challenges-in-building-enterprise-llm-applications/',\n",
              "     'content': 'Operational Challenges\\n----------------------\\n\\nDeploying and maintaining large language models (LLM) systems in production environments presents several operational challenges. These challenges span from the initial deployment to ongoing maintenance and cost management.\\n\\n### Deployment and Maintenan...',\n",
              "     'relevance_score': 0.7931224},\n",
              "    {'title': 'Cost-Effective Deployment of Large LLMs: Overcoming Infrastructure ...',\n",
              "     'url': 'https://wallaroo.ai/cost-effective-deployment-of-large-llms-overcoming-infrastructure-constraints/',\n",
              "     'content': 'Deploying large language models (LLMs) at scale presents unique infrastructure challenges, especially concerning the cost and efficiency of managing such models in production. These demands often strain infrastructure, and drive up operational expenses for production LLMs. For organizations deployin...',\n",
              "     'relevance_score': 0.78963995}],\n",
              "   'confidence': 'high',\n",
              "   'num_sources': 5},\n",
              "  'How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?': {'question': 'How are different industries (e.g., healthcare, finance, education) leveraging LLM integration to improve efficiency and create new functionalities?',\n",
              "   'answer': 'Large language models (LLMs) are rapidly transforming various industries by enhancing efficiency and creating novel functionalities.  Across healthcare, finance, and education, LLMs are automating tasks, improving decision-making, and personalizing service delivery.  In healthcare, LLMs can automate administrative tasks, analyze medical records to assist with diagnosis, and personalize patient care plans, ultimately improving workflows and reducing errors (Source 1).  Similarly, in finance, LLMs can streamline processes like fraud detection, risk assessment, and customer service, leading to improved efficiency and reduced operational costs (Source 1, Source 2).  The education sector benefits from LLMs through personalized learning experiences, automated grading, and the creation of intelligent tutoring systems (Source 1, Source 2).\\n\\nSource 2 highlights the potential of integrating LLMs with federated learning (FedLLMs) to address challenges related to sensitive data in these industries.  This approach allows for the training and application of powerful models without relying on centralized data storage, thus enhancing security and scalability while potentially promoting more equitable access to LLM technology.  The integration of LLMs and federated learning is presented as a transformative technology, particularly for industries handling sensitive data (Source 2).\\n\\nFurthermore, platforms like LeewayHertz\\'s ZBrain demonstrate the practical application of LLMs in creating custom applications tailored to specific business needs (Source 3).  ZBrain leverages various advanced language models (GPT, Llama, PaLM, etc.) to process diverse data types and build custom \"Flows,\" complex business logic forming the foundation of LLM-based applications.  This approach not only enhances precision but also minimizes errors and addresses challenges such as inconsistent supplier performance and prolonged decision-making (Source 3).  These customized applications are crucial for optimizing operational workflows and improving customer service across diverse industries.  While specific statistics or quantifiable improvements are not provided in the sources, the overall consensus points to significant potential for efficiency gains and functional enhancements through LLM integration in healthcare, finance, and education.',\n",
              "   'sources': [{'title': 'Direct Answer Summary',\n",
              "     'url': 'Tavily AI Summary',\n",
              "     'content': 'Large language models (LLMs) enhance efficiency in healthcare, finance, and education by automating tasks, improving decision-making, and creating customized applications for better service delivery. They optimize workflows and reduce errors.',\n",
              "     'relevance_score': 1.0},\n",
              "    {'title': 'Review Integration of large language models and federated learning',\n",
              "     'url': 'https://www.sciencedirect.com/science/article/pii/S2666389924002708',\n",
              "     'content': 'In light of the inherent distinguished characteristics of FedLLMs, we explore the broad range of application fields for FedLLMs. These applications mainly include healthcare, finance, education, and so on. Within these scenarios, deploying FedLLMs has the potential to address real industry problems,...',\n",
              "     'relevance_score': 0.73505294},\n",
              "    {'title': 'AI Use Cases & Applications Across Major industries - LeewayHertz',\n",
              "     'url': 'https://www.leewayhertz.com/ai-use-cases-and-applications/',\n",
              "     'content': '[ZBrain](https://zbrain.ai/), LeewayHertzâ€™s enterprise AI platform, significantly enhances operational workflows within businesses across industries. The platform creates custom LLM-based applications tailored to that integrate with clientsâ€™ proprietary data, improving their operational efficiency a...',\n",
              "     'relevance_score': 0.47022632}],\n",
              "   'confidence': 'high',\n",
              "   'num_sources': 3},\n",
              "  'What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?': {'question': 'What are the ethical considerations and potential biases associated with deploying LLMs in various applications, and how can these be mitigated?',\n",
              "   'answer': 'Deploying Large Language Models (LLMs) presents significant ethical considerations and potential biases that demand careful mitigation.  A primary concern is bias, stemming from the vast datasets used for training. These datasets, often sourced from the internet, can reflect and amplify existing societal biases, leading to outputs that perpetuate stereotypes or discriminate against certain groups (Source 2).  This bias can manifest in various ways, impacting different applications differently. For example, in healthcare, biased LLMs could negatively influence the quality of care provided to specific patient groups (Source 4).  Mitigation strategies include employing diverse and representative datasets during training (Source 3), implementing techniques for identifying and mitigating bias during model development (Source 2), and conducting regular audits of LLM outputs to ensure fairness and ethical standards (Source 3).\\n\\nPrivacy is another critical concern.  The use of personal data in training and application necessitates robust data protection measures to prevent unauthorized access or misuse (Source 1).  Transparency is crucial for building user trust and enabling informed usage.  Developers should disclose the training data, processes, and potential biases (Source 3), fostering accountability.  Without transparency, it becomes difficult to assess and address potential harms. Source 2 emphasizes the need for transparency regarding model capabilities, limitations, and training data.\\n\\nAccountability is also paramount.  Determining responsibility for LLM outputs and their consequences is a complex challenge.  Source 4 discusses the challenges of clinical accountability when using LLMs in medical practice.  Establishing independent ethics review boards to assess LLM development plans before deployment could help mitigate risks and ensure alignment with societal values (Source 3). The potential for misuse of LLMs, such as generating misinformation or harmful content, necessitates careful consideration of safeguards and responsible deployment practices.  While LLMs offer potential benefits across various applications, these ethical considerations must be proactively addressed to ensure their responsible and equitable implementation.  Addressing bias, protecting privacy, ensuring transparency, and establishing accountability mechanisms are vital steps in mitigating the risks associated with LLM deployment.',\n",
              "   'sources': [{'title': 'Direct Answer Summary',\n",
              "     'url': 'Tavily AI Summary',\n",
              "     'content': 'Ethical considerations for LLMs include bias mitigation, privacy, and transparency. Bias can perpetuate stereotypes; privacy concerns arise from data use. Mitigation strategies include diverse data and regular audits.',\n",
              "     'relevance_score': 1.0},\n",
              "    {'title': 'Ethical Considerations in LLM Development and Deployment',\n",
              "     'url': 'https://dev.to/nareshnishad/ethical-considerations-in-llm-development-and-deployment-3j6n',\n",
              "     'content': '## Key Ethical Challenges in LLM Development and Deployment\\n\\n### 1. Bias and Fairness\\n\\nLLMs are trained on vast amounts of data, often sourced from the internet, which may contain inherent biases. If not addressed, these biases can be amplified, resulting in outputs that perpetuate stereotypes or ma...',\n",
              "     'relevance_score': 0.86954874},\n",
              "    {'title': 'What are Ethics and Bias in LLMs? - AI Agent Builder',\n",
              "     'url': 'https://www.appypieagents.ai/blog/ethics-and-bias-in-llms',\n",
              "     'content': 'Examples of current regulations, such as data protection laws, guidelines on AI transparency, and standards for algorithmic accountability, are examined.Policy Recommendations for Addressing Bias and Ethics in LLMsBuilding upon the understanding of the regulatory landscape, this subsection proposes ...',\n",
              "     'relevance_score': 0.8660535},\n",
              "    {'title': 'Implications of Large Language Models for Clinical Practice: Ethical ...',\n",
              "     'url': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11609490/',\n",
              "     'content': 'which even highly trained medical practitioners might have the quality of their care improved. With regard to nonâ€maleficence, LLMs can cause harm through training set bias, uncertainties around clinical accountability, data privacy issues, and insufficient trustworthiness (through poor transparency...',\n",
              "     'relevance_score': 0.8098936},\n",
              "    {'title': 'Ethical Considerations and Bias Mitigation in Large Language ...',\n",
              "     'url': 'https://www.researchgate.net/publication/387295166_Ethical_Considerations_and_Bias_Mitigation_in_Large_Language_Models_AUTHOR',\n",
              "     'content': 'This paper explores the ethical implications associated with LLMs, focusing on issues such as fairness, transparency, accountability, and the',\n",
              "     'relevance_score': 0.80442166}],\n",
              "   'confidence': 'high',\n",
              "   'num_sources': 5},\n",
              "  'What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?': {'question': 'What are the latest research trends and advancements in LLM integration techniques, focusing on areas such as prompt engineering and fine-tuning for specific tasks?',\n",
              "   'answer': 'Recent research in LLM integration centers on enhancing performance and accessibility through improved prompt engineering and fine-tuning techniques.  Prompt engineering is evolving beyond simple input phrasing.  Source 2 highlights \"iterative refinement,\" where prompts are repeatedly tested and adjusted, and \"context layering,\" where crucial information is embedded within the prompt to direct the LLM\\'s response.  These methods, applicable to diverse tasks from creative writing to code assistance, are increasingly crucial for optimizing LLM outputs across various industries.\\n\\nFine-tuning methods are also experiencing significant advancements.  Sources 1 and 3 identify \"prompt tuning\" as a key trend, focusing on refining prompts to improve model responses. This approach often complements traditional fine-tuning, offering a balance between performance gains and computational efficiency.  The automation of fine-tuning through AutoML (Sources 1 and 3) is accelerating the process and broadening access for researchers and developers.  AutoML automates tasks such as hyperparameter selection and optimization.\\n\\nZero-shot and few-shot learning (Sources 3 and 4) are emerging as significant LLM fine-tuning techniques, enabling models to handle tasks with minimal or no task-specific data. While promising, challenges remain, including potential overfitting and limitations in evaluation methodologies.  Source 4 discusses the application of few-shot prompt engineering and fine-tuning in Amazon Bedrock, indicating its practical implementation in real-world applications.\\n\\nThe trend towards task specialization and fine-tuning is also prominent, exemplified by models like Med-PaLM 2 and Radiology-Llama2 (Source 5).  These models, fine-tuned with domain-specific data, demonstrate enhanced accuracy and context relevance within specialized fields like healthcare and radiology.  This targeted fine-tuning represents a move away from general-purpose LLMs towards models optimized for specific tasks and industries, showcasing the growing sophistication of LLM integration techniques.  Overall, research suggests a convergence of improved prompt engineering methodologies with increasingly efficient and specialized fine-tuning approaches, driving the expansion and utility of LLMs across diverse domains.',\n",
              "   'sources': [{'title': 'Direct Answer Summary',\n",
              "     'url': 'Tavily AI Summary',\n",
              "     'content': 'Latest research trends focus on prompt tuning and zero-shot learning for LLMs, with fine-tuning for domain-specific applications. Prompt engineering is increasingly refined for diverse tasks. AutoML aids in automating fine-tuning processes.',\n",
              "     'relevance_score': 1.0},\n",
              "    {'title': 'The Future of LLM Prompt Engineering: Trends and Innovation',\n",
              "     'url': 'https://mindy-support.com/news-post/the-future-of-llm-prompt-engineering-trends-and-innovation/',\n",
              "     'content': 'Prompt engineering techniques include iterative refinement, where prompts are tested and adjusted for better results, and context layering, where specific information is embedded to guide the AIâ€™s responses. These techniques support diverse use cases, from generating creative content and summarizing...',\n",
              "     'relevance_score': 0.7574541},\n",
              "    {'title': '18 Artificial Intelligence LLM Trends in 2025 | AI Bistrot - Medium',\n",
              "     'url': 'https://medium.com/data-bistrot/15-artificial-intelligence-llm-trends-in-2024-618a058c9fdf',\n",
              "     'content': '**Prompt tuning**, which focuses on refining input prompts to enhance model responses, is gaining traction. It is increasingly used in combination with traditional fine-tuning methods to strike a balance between performance and computational efficiency.\\n\\n**AutoML** is revolutionizing LLM fine-tuning...',\n",
              "     'relevance_score': 0.6922474},\n",
              "    {'title': 'Few-shot prompt engineering and fine-tuning for LLMs in Amazon ...',\n",
              "     'url': 'https://aws.amazon.com/blogs/machine-learning/few-shot-prompt-engineering-and-fine-tuning-for-llms-in-amazon-bedrock/',\n",
              "     'content': 'an apples-to-oranges comparison. As LLMs continue to advance, we anticipate further improvements in their output quality. [...] evaluate based on their specific needs and priorities. As language models continue advancing, further research in customizing and refining these models for the financial se...',\n",
              "     'relevance_score': 0.6847074},\n",
              "    {'title': 'The Future of Large Language Models in 2025 - Research AIMultiple',\n",
              "     'url': 'https://research.aimultiple.com/future-of-large-language-models/',\n",
              "     'content': '**Task specialization and fine-tuning**: LLMs are now being fine-tuned for specific domains, such as healthcare (e.g., **Med-PaLM 2**), law, and science. Models like **Radiology-Llama2** and **MedAlpaca** are fine-tuned with domain-specific data, allowing for more accurate and context-relevant outpu...',\n",
              "     'relevance_score': 0.64880073}],\n",
              "   'confidence': 'high',\n",
              "   'num_sources': 5}},\n",
              " 'summary': {'total_questions': 5,\n",
              "  'total_sources': 23,\n",
              "  'high_confidence_answers': 5,\n",
              "  'timestamp': '2025-06-13 07:52:41'}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}